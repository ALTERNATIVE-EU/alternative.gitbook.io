{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ALTERNATIVE Platform - User Guide","text":"<p>The web platform, made with CKAN, can be used to manage, publish and find data. It also has an API, which can be used by generating an API Token; and a python library - for searching datasets and downloading resources.</p>"},{"location":"#contents","title":"Contents:","text":"<p>Users, Organizations and Authorization</p> <p>Datasets, Resources and Groups</p> <p>Metadata</p> <p>JupyterHub</p>"},{"location":"datasets/","title":"Datasets, Resources and Groups","text":"<p>Data is published in units called datasets. A dataset contains: information or metadata about the data; a number of resources, which hold the data itself. They are stored in an S3 bucket in Google Cloud Storage, or simply as a link, if the resource is elsewhere on the web.</p>"},{"location":"datasets/#exploring-datasets","title":"Exploring Datasets","text":"<p>By selecting <code>Datasets</code> you can see a list of all datasets on the platform. From the datasets page or an organization's page you can find a dataset you are interested in (more details about searching datasets). Selecting it will display the dataset page. At the top there are 3 tabs: - <code>Dataset</code> - here you can see all the information about the dataset and a list of its resources, choosing a resource will take you to its page, where you can see details about it, manage and download it - <code>Groups</code> - see any group the dataset is part of, or add it to new groups by selecting the group name and pressing <code>Add to group</code> - <code>Activity Stream</code> - see the history of changes made to the dataset</p>"},{"location":"datasets/#creating-a-new-dataset","title":"Creating a New Dataset","text":"<ol> <li>You can access the <code>Create Dataset</code> screen in two ways. Go to <code>Datasets</code> page, then select the <code>Add Dataset</code> button. Alternatively, go to <code>Organizations</code>, select the page for the organization that should own your new dataset. Provided that you are a member of this organization, you can now select the <code>Add Dataset</code> button.</li> <li>Add information about the data</li> <li>Press <code>Next: Add Data</code></li> <li>This is where you will add one or more resources which contain the data for this dataset. Choose a file (<code>Upload</code> button, max file size = 1 GB) or link (<code>Link</code> button) for your data resource. Fill the other information on the page:</li> <li>Name - a name for this resource, different resources in the dataset should have different names</li> <li>Description - a description of the resource</li> <li>Format - the file format of the resource</li> <li>If you have more resources to add to the dataset, select <code>Save &amp; add another</code>. When you have added all resources, press <code>Finish</code>.</li> </ol>"},{"location":"datasets/#managing-dataset","title":"Managing Dataset","text":"<p>You can edit the dataset you have created, or any dataset owned by an organization that you are a member of, or any dataset, in which you have been listed as a collaborator with the Editor role or higher.</p> <p>Go to the dataset\u2019s page. Select <code>Manage</code>. Here you can:</p>"},{"location":"datasets/#edit-metadata-or-delete-dataset","title":"Edit Metadata or Delete Dataset","text":"<p>In the <code>Edit metadata</code> tab you can edit any of the fields. When you have finished, press the <code>Update Dataset</code> button to save your changes. Alernatively, you can delete the dataset by pressing <code>Delete</code>. The dataset is not completely deleted. It is hidden, so it does not show up in any searches. However, by visiting the URL for the dataset\u2019s page, it can still be seen (by users with appropriate authorization), and undeleted if necessary. If it's important to completely delete the dataset - contact a sysadmin user.</p>"},{"location":"datasets/#manage-a-datasets-resources","title":"Manage a Dataset's Resources","text":"<p>In the <code>Resources</code> tab, you can add new resources to the dataset by pressing <code>Add new resource</code>, or, by selecting a resource, you can edit information about it. When you have finished editing, select the button marked <code>Update Resource</code> to save your changes. To delete the resource, press <code>Delete</code>.</p>"},{"location":"datasets/#dataset-collaborators","title":"Dataset Collaborators","text":"<p>In the <code>Collaborators</code> tab, you'll see a list of all users that have been given special permissions to the dataset (collaborators don't have to be members of the organization that owns the dataset). From the list you can <code>Edit</code> (change their role) or <code>Delete</code> a collaborator. Add new collaborator by selecting <code>Add Collaborators</code> - choose their username, assign a role and press <code>Add Collaborator</code>.</p> <p>Collaborator Roles:</p> <ul> <li>Member - can access the dataset if private, but not edit it</li> <li>Editor - can edit the dataset and its resources, as well as accessing the dataset if private</li> <li>Admin - in addition to managing the dataset, they can add and remove collaborators</li> </ul>"},{"location":"datasets/#groups","title":"Groups","text":"<p>Groups are collections of datasets (datasets in a group can be owned by different organizations). They can also have members. By selecting <code>Groups</code> near the top of any page, you can find all the existing groups. Selecting a group will take you to its page where you'll find information about it and datasets that have been added to the group. Only sysadmin users can create groups, by going to the <code>Groups</code> page and pressing <code>Add Group</code>. Initially, the group has no datasets and only 1 member - the creator.</p>"},{"location":"datasets/#management","title":"Management","text":"<p>From the group's page, select <code>Manage</code>. On this page there are 2 tabs: <code>Edit</code> - where you can change information about the group or <code>Delete</code> it; and <code>Members</code> - you can see a list of all members, add or remove users from the group, or change their role.</p> <p>Group Roles:</p> <ul> <li>Member - can add/remove datasets from the group</li> <li>Admin - do the same as a member + can edit group information, as well as manage the group's members</li> </ul>"},{"location":"jupyter/","title":"JupyterHub","text":"<p>The platform integrates JupyterHub as a way to interact with datasets and virtual environments. To access JupyterHub select the <code>Jupyterhub</code> button from the top right corner of any page. Everytime a user visits the JupyterHub page, a JupyterLab server is spawned for them, which enables you to work with documents and activities such as Jupyter notebooks, text editors, files and terminals. Jupyter Notebooks (.ipynb files) are documents that combine runnable code with narrative text (Markdown), equations (LaTeX), images, interactive visualizations and more. Once your server is ready, you will be redirected to it. After some time of inactivity it will be shutdown.</p>"},{"location":"jupyter/#jupyterlab","title":"JupyterLab","text":"<p>At the top you can find different options and settings. You can get back to JupyterHub by selecting <code>File</code> -&gt; <code>Hub Control Panel</code> or press <code>Log Out</code> to end the session. At the bottom you can see how many terminals and consoles are open and what environment is being used. On the right there are debugging tools.</p> <p>On the left there's 4 tabs: - <code>File Browser</code> - interact with files or open a <code>Launcher</code> tab from the <code>+</code> button - <code>Running Terminals and Kernels</code> - see all open tabs and running kernels and terminals, option to close or stop any/all of them - <code>Table of Contents</code> - shows information about the currently open file/notebook - <code>Extension Manager</code> - can be used to install extensions</p> <p>In the middle, you can see the currently open tabs and interact with them. From the <code>Launcher</code> tab (create new one by clicking <code>+</code>), you can create notebooks or consoles, open terminals or start new text, markdown and python files. Example of a notebook file.</p>"},{"location":"jupyter/#file-system","title":"File System","text":"<p>The <code>/home/jovyan</code> and <code>/home/shared</code> directories are not being deleted between server stops. <code>/home/jovyan</code> is personal for the user - nobody else can see its content; and has 10 GB available space. <code>/home/shared</code> is for all users - everybody can see its content; and has 20 GB available space. All other directories are deleted and recreated everytime the server is stopped/started. The folder <code>/home/jovyan/shared</code> is a symlink to <code>/home/shared</code> and can be used to interact with the files of the shared directory.</p>"},{"location":"jupyter/#environments","title":"Environments","text":"<p>Every JupyterLab server is being spawned with two environments from the start - default python environment and an additional conda environment. On <code>Launcher</code> tab under <code>Notebook</code> or <code>Console</code> you can choose which environment to use. You can also use the <code>Terminal</code>:</p>"},{"location":"jupyter/#python-virtual-environments","title":"Python Virtual Environments","text":"<p>Each environment has their own independent set of Python packages installed in their <code>site</code> directories. A virtual environment is created on top of an existing Python installation, known as the virtual environment\u2019s <code>base</code> Python, and may be isolated from the packages in the base environment, so only those explicitly installed in the virtual environment are available. Python Virtual Environment Docs.</p> <p>Make sure you create these environments in the <code>/home/jovyan</code> directory or they will get deleted.</p> <ul> <li> <p>Add New Environments: <pre><code>python -m venv /path/to/new/virtual/environment\n</code></pre></p> </li> <li> <p>Activate Different Environment: <pre><code>source environment/bin/activate\n</code></pre></p> </li> <li> <p>Install Packages: <pre><code>pip install packageName\n</code></pre></p> </li> <li> <p>Uninstall Packages: <pre><code>pip uninstall packageName\n</code></pre></p> </li> <li> <p>Show Packages In Environment: <pre><code>pip list\n</code></pre></p> </li> <li> <p>Deactivate Current Environment: <pre><code>deactivate\n</code></pre></p> </li> <li> <p>Delete Environment: <pre><code>rm -rf environment\n</code></pre></p> </li> </ul>"},{"location":"jupyter/#conda-environments","title":"Conda Environments","text":"<p>Each environment has their own independent set of Python or Conda packages installed. Conda Environments Docs. Guide - using <code>pip</code> inside conda environment.</p> <p>Make sure you create these environments in the <code>/home/jovyan</code> directory or they will get deleted.</p> <p>You can check environment names and locations with: <pre><code>bash\nconda env list\n</code></pre></p> <ul> <li> <p>Add New Environments: <pre><code>conda create --prefix /home/jovyan/Conda2\n</code></pre></p> </li> <li> <p>Activate Different Environment: <pre><code>conda activate /home/jovyan/Conda2\n</code></pre></p> </li> <li> <p>Install Packages: <pre><code>conda install pip\npip install packageName\n</code></pre></p> </li> <li> <p>Uninstall Packages: <pre><code>pip uninstall packageName\n</code></pre></p> </li> <li> <p>Show Packages In Environment: <pre><code>conda list\npip list\n</code></pre></p> </li> <li> <p>Create Kernel From Environment: <pre><code>pip install ipykernel\npython -m ipykernel install --user --name=Conda2\n</code></pre></p> </li> <li> <p>Deactivate Current Environment: <pre><code>conda deactivate\n</code></pre></p> </li> <li> <p>Delete Environment: <pre><code>conda activate base\nconda remove -p /home/jovyan/Conda2 --all\njupyter kernelspec remove conda2\n</code></pre></p> </li> </ul>"},{"location":"jupyter/#python-library","title":"Python Library","text":"<p>The python library alternative-lib is designed to help with finding datasets and downloading resources from them, by using ckanapi.</p> <p>It can be installed with: <pre><code>pip install alternative-lib\n</code></pre></p> <p>Example of the library being used to get a public dataset and download its resource.</p>"},{"location":"jupyter/#r","title":"R","text":"<p>You can use R from a <code>Terminal</code>. The first time you install a new package you should be asked where to save it. Make sure any packages you install are in the <code>/home/jovyan</code> directory or they will get deleted.</p> <ul> <li> <p>Activate R Console: <pre><code>R\n</code></pre></p> </li> <li> <p>Help Command: <pre><code>help()\n</code></pre></p> </li> <li> <p>Install Package: <pre><code>install.packages(\"package_name\")\n</code></pre></p> </li> <li> <p>List Installed Packages: <pre><code>installed.packages()\n</code></pre></p> </li> <li> <p>Exit R Console: <pre><code>q()\n</code></pre></p> </li> </ul>"},{"location":"jupyter/#useful-r-docs","title":"Useful R Docs","text":"<ul> <li>The R Project</li> <li>Kickstarting R</li> <li>Installing R Packages</li> </ul>"},{"location":"jupyter/#version-update","title":"Version Update","text":"<p>The R version has been updated from 4.0.4 to 4.3.0. If you have installed any R packages before this update, you will need to add your previous package directory to R's paths, to be able to use those packages.</p> <p>Commands (should be executed in R console; replace <code>yourLib</code> with the path to your R package directory; by default that should have been <code>~/R/x86_64-pc-linux-gnu-library/4.0</code>):</p> <p>To add the directory in paths list: <pre><code>.libPaths( c( .libPaths(), \"yourLib\") )\n</code></pre></p> <p>To make the directory the main library for packages: <pre><code>.libPaths( c( \"yourLib\" , .libPaths() ) )\n</code></pre></p>"},{"location":"jupyter/#bioconductor","title":"Bioconductor","text":"<p>Installing and Using Bioconductor. When installing packages with <code>BiocManager::install()</code>, add the path to your R package directory, like so (replace <code>your_R_package_dir</code> with the directory name): <pre><code>BiocManager::install(lib=\"/home/jovyan/your_R_package_dir\")\n</code></pre> or <pre><code>BiocManager::install(\"package_name\", lib=\"/home/jovyan/your_R_package_dir\")\n</code></pre></p>"},{"location":"metadata/","title":"Metadata","text":"<p>Each dataset has metadata associated with it. Metadata is structured reference data that helps to sort and identify the information it describes.</p>"},{"location":"metadata/#fields","title":"Fields","text":"<ul> <li>Title - this title will be unique across the platform, so make it brief but specific</li> <li>URL - automatically filled based on title, but can be edited</li> <li>Description - add a longer description of the dataset, including information such as what people need to know when using the data</li> <li>Tags - add tags that will help people find the data and link it with other related data</li> <li>License - it is important to include a license, so that people know how they can use the data</li> <li>Organization - choose which organization, that you're a member of, should own the dataset</li> <li>Visibility - a public dataset can be seen by anyone, a private one can only be seen by members of the organization owning the dataset or by collaborators of the dataset, and will not show up in searches by other users</li> <li>Source - where the data is from</li> <li>Version - version of the data</li> <li>Author - name of the person or organization responsible for producing the data</li> <li>Author e-mail - an e-mail address for the author, to which queries about the data should be sent</li> <li>Maintainer - name of the person or organization responsible for maintaining the data</li> <li>Maintainer e-mail - details for a second person responsible for the data</li> </ul>"},{"location":"metadata/#advanced-metadata-for-experiments","title":"Advanced Metadata for Experiments","text":"<p>When creating/editing a dataset, you can mark the checkbox to be able to set extra fields, related to experiment data:</p> <ul> <li>Culture medium</li> <li>Number of replicates</li> <li>Number of cells/well</li> <li>Ratio hiPSC-CMs/HCAECs</li> <li>Date of experiment</li> <li>Toxin</li> <li>Age type</li> <li>Dimension</li> <li>Category</li> <li>Content</li> <li>Model</li> </ul>"},{"location":"metadata/#custom-fields","title":"Custom Fields","text":"<p>If you want the dataset metadata to have more fields, you can add a name/key and value for it.</p>"},{"location":"metadata/#find-data","title":"Find Data","text":"<p>You should be able to find a dataset by typing the title, or some relevant words from the description/metadata, into the search box on any page, containing datasets. On the left side of the <code>Datasets</code> page there are also some filters, such as <code>Organizations</code>, <code>Groups</code>, <code>Tags</code>, <code>Formats</code>, <code>Licenses</code>. Select any number of options to restrict the search. Under the search box there's also an <code>Order by</code> field, to sort datasets in any given way. If you want to look for data owned by a particular organization/group, you can search within that from its homepage.</p> <p>CKAN uses Apache Solr as its search engine. Note that not the whole functionality is offered through the simplified search interface in CKAN or it can differ. CKAN supports two search modes, both are used from the same search field. If the search terms entered into the search field contain no colon (<code>:</code>) CKAN will perform a simple search. If the search expression does contain a colon CKAN will perform an advanced search.</p>"},{"location":"metadata/#simple-search","title":"Simple Search","text":"<p>CKAN defers most of the search to Solr and by default it uses the DisMax Query Parser that was primarily designed to be easy to use and to accept almost any input.</p> <p>The search words typed by the user in the search box defines the main query constituting the essence of the search. Some characters are treated as mandatory (<code>+</code>) and prohibited (<code>-</code>) modifiers for terms. Text wrapped in balanced quote characters (<code>\u201cexample text\u201d</code>) is treated as a phrase. By default, all words or phrases specified by the user are treated as optional unless they are preceded by <code>+</code> or <code>-</code>. CKAN will search for the complete word and when doing simple search wildcards are not supported. Solr applies some preprocessing and stemming when searching. Stemmers remove morphological affixes from words, leaving only the word stem. This may cause, for example, that searching for <code>testing</code> or <code>tested</code> will show also results containing the word <code>test</code>. If the name of the dataset contains words separated by <code>-</code> it will consider each word independently in the search.</p> <p>Examples:</p> <ul> <li><code>census</code> -&gt; will search for all the datasets containing the word <code>census</code> in the query fields</li> <li><code>census +2019</code> -&gt; will search for all the datasets contaning the word <code>census</code> and filter only those matching <code>2019</code> too</li> <li><code>census -2019</code> -&gt; will search for all the datasets containing the word <code>census</code> and will exclude the results featuring <code>2019</code></li> <li><code>\"european census\"</code> -&gt; will search for all the datasets containing the phrase <code>european census</code></li> <li><code>Testing</code> -&gt; will search for all the datasets containing the word <code>Testing</code> and also <code>Test</code> as it is the stem of <code>Testing</code></li> </ul>"},{"location":"metadata/#advanced-search","title":"Advanced Search","text":"<p>This will be considered a fielded search and the query syntax of Solr will be used to search. This will allow us to use wildcards (<code>*</code>), proximity matching (<code>~</code>, looks for terms that are within a specific distance from one another) and general features described in Solr docs. The basic syntax is <code>field:term</code>. Field names may differ from datasets' attributes, the mapping rules are defined in the schema.xml file. You can use <code>title</code> to search by the dataset name and <code>text</code> to look in a catch-all field. CKAN supports fuzzy searches based on the Levenshtein Distance, or Edit Distance algorithm, to do a fuzzy search use the <code>~</code> symbol at the end of a single-word term.</p> <p>Examples:</p> <ul> <li><code>title:european</code> -&gt; will look for all the datasets containing in their title the word <code>european</code></li> <li><code>title:europ*</code> -&gt; will look for all the datasets containing in their title a word that starts with <code>europ</code>, like <code>europe</code> and <code>european</code></li> <li><code>title:europe || title:australia</code> -&gt; will look for datasets containing <code>europe</code> or <code>australia</code> in their title</li> <li><code>title: \"european census\" ~ 4</code> -&gt; will look for datasets, in which the title contains the words <code>european</code> and <code>census</code> within a distance of 4 words</li> <li><code>author:powell~</code> -&gt; words like <code>jowell</code> or <code>pomell</code> will also be found</li> </ul>"},{"location":"users/","title":"Users, Organizations and Authorization","text":"<p>There are 2 types of users: regular and sysadmin. An account is not required to search for and find data, unless the information is private, but it is needed for all publishing functions and personalization features. The platform uses Keycloak to manage user identity and access. You can find the <code>Log in</code> button from the top right corner of any page.</p>"},{"location":"users/#profile","title":"Profile","text":"<p>You can see your profile by selecting the button containing your picture and name at the top of any page. You can change the information about you, including what other users see about you by editing your profile. To do this, select the gearwheel symbol at the top of any page or go to your profile page and select <code>Manage</code>. Make the changes you want and then press <code>Update Profile</code>.</p>"},{"location":"users/#api-tokens","title":"API Tokens","text":"<p>API tokens are used in API calls or with the python library to access private datasets, that your account has rights to.</p> <p>To generate an API token: 1. Go to your profile page 2. Select the <code>API Tokens</code> tab 3. Set a name for your token and click <code>Create API Token</code> 4. A green box appears above, containing your API token, make sure to copy it - you won't be able to see it again</p> <p>Underneath the creation form you can see all your tokens. From the list you can also <code>Revoke</code> a token, that you don't need anymore.</p>"},{"location":"users/#news-feed","title":"News Feed","text":"<p>At the top of any page, select the dashboard symbol. This shows changes to datasets, organizations and groups that you follow. It is possible to follow individual users to be notified of changes that they make. To start/stop following something, go to its dedicated page and select <code>Follow</code>/<code>Unfollow</code>. You can also select the <code>Activity Stream</code> tab to see all activities related to the object. You can enable email notifications for updates to things you follow by enabling <code>Subscribe to notification emails</code> from your profile settings.</p>"},{"location":"users/#organizations","title":"Organizations","text":"<p>Organizations have members and own datasets. You need to be a member of an organization in order to create datasets. Each organization can have its own workflow and authorizations, allowing it to manage its own publishing process. It also has a homepage, where users can find some information about the organization and search within its datasets, this can be accessed by going to <code>Organizations</code> and selecting the specific organization you want to explore. Only sysadmin users can create organizations, by going to the <code>Organizations</code> page and pressing <code>Add Organization</code>. Initially, the organization has no datasets and only 1 member - the creator.</p>"},{"location":"users/#management","title":"Management","text":"<p>Users with the Admin role can edit an organization's information and change the access privileges to the organization for other users. You can do so by going to the organization\u2019s page and selecting the <code>Manage</code> button. The <code>Edit</code> tab lets you change organization information or <code>Delete</code> it. From the <code>Members</code> tab you can see a list of all members, add or remove users from the organization, or change their role.</p>"},{"location":"users/#user-roles","title":"User Roles","text":"<p>A user in an organization can create a dataset owned by that organization.</p> <ul> <li>Member - can see the organization\u2019s private datasets</li> <li>Editor - anything a member can do + can edit and publish datasets</li> <li>Admin - anything an editor can do + can add, remove and change roles of organization members</li> </ul>"},{"location":"ai-ml-api/","title":"AI/ML API Server","text":"<p>Welcome to the documentation for the AI/ML API Server. This server a part of the ALTERNATIVE project, providing a robust and secure interface for accessing a wide range of machine learning models. Our goal is to simplify the integration of ML models into various applications and workflows, ensuring seamless access and efficient operation.</p>"},{"location":"ai-ml-api/#overview","title":"Overview","text":"<p>The AI/ML API Server is engineered to support high-demand scenarios, offering a unified interface for machine learning models developed by consortium partners. It ensures seamless integration, secure access, and efficient operation, catering to a variety of use cases from predictive analytics to real-time data processing.</p>"},{"location":"ai-ml-api/#key-objectives","title":"Key Objectives","text":"<ul> <li>Seamless Integration: Simplify the incorporation of ML models into existing applications and workflows.</li> <li>Secure API Access: Implement state-of-the-art security measures for data protection and access control.</li> <li>Scalable Architecture: Dynamically adjust resources to handle varying loads, ensuring consistent performance.</li> <li>High Availability: Design for fault tolerance and resilience to minimize downtime.</li> <li>Comprehensive Documentation: Provide detailed guides and examples to facilitate easy adoption.</li> <li>User-Friendly Interfaces: Offer intuitive tools for managing API tokens and accessing model functionalities.</li> </ul>"},{"location":"ai-ml-api/#features","title":"Features","text":"<ul> <li>Diverse Model Support: Access a wide range of ML models for different domains and applications.</li> <li>Token-Based Authentication: Secure API endpoints with robust token authentication mechanisms.</li> <li>Scalable Deployment: Leverage Docker and Kubernetes for scalable and manageable deployments.</li> <li>Performance Monitoring: Integrated tools for tracking API performance and usage statistics.</li> <li>Interactive Documentation: Explore API functionalities with interactive Swagger documentation.</li> </ul>"},{"location":"ai-ml-api/deployment/","title":"Deployment Process","text":""},{"location":"ai-ml-api/deployment/#production-deployment","title":"Production Deployment:","text":"<p>For the production deployment, we use Kubernetes to manage the deployment of the AI/ML API server and its associated components. The deployment process involves the following steps:</p>"},{"location":"ai-ml-api/deployment/#aiml-api-server-deployment","title":"AI/ML API Server Deployment:","text":"<ol> <li>Build Docker Image</li> <li>Push Image to Registry</li> <li>Update Deployment Files</li> <li>Apply Deployment</li> </ol> <p>In the project README, we provide detailed instructions for each step of the deployment process, including the commands to execute.</p>"},{"location":"ai-ml-api/deployment/#custom-envoy-filter-deployment","title":"Custom Envoy Filter Deployment:","text":"<ol> <li>Apply the SQL Schema</li> <li>Build the Filter</li> <li>Create PVC</li> <li>Add volume and volume mount to the sidecar container</li> <li>Copy the filter binary to the volume</li> <li>Update the Envoy Filter manifest configuration</li> <li>Apply the Envoy Filter manifest</li> </ol>"},{"location":"ai-ml-api/repositories/","title":"Repositories","text":"Repository Description Link AI/ML API Core REST API component with Flask application logic GitHub Envoy Filter Custom Filter Go implementation of token-based authentication and authorization GitHub Keycloak AI/ML Access Tokens Plugin Keycloak extension for managing AI/ML API access tokens GitHub CKAN Platform Modified CKAN platform with token management features GitHub Keycloak Authorization Plugin Keycloak plugin for managing user roles and permissions GitHub"},{"location":"ai-ml-api/scalability/","title":"Scalability","text":"<p>To accommodate varying loads and optimize resource utilization, our system employs a multi-faceted approach to scalability. This document outlines the key strategies used to ensure that the system can handle growth in demand without compromising on performance.</p>"},{"location":"ai-ml-api/scalability/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>Horizontal scaling, also known as scaling out, involves adding more instances of the API servers to distribute the load evenly. This approach is particularly effective for handling an increase in user requests.</p> <ul> <li> <p>Stateless API Servers: Our API servers are designed to be stateless, which means they do not store any user data between requests. This design choice allows us to add or remove server instances without impacting the system's state or performance.</p> </li> <li> <p>Kubernetes Horizontal Node Scaling: We leverage on the Cloud Provider's Kubernetes environment to automatically scale the number of nodes based on resource utilization. This ensures that the system can handle varying loads efficiently.</p> </li> </ul>"},{"location":"ai-ml-api/scalability/#benefits-of-horizontal-scaling","title":"Benefits of Horizontal Scaling","text":"<ul> <li>Flexibility: Easily adjust capacity by adding or removing instances.</li> <li>Fault Tolerance: Reduced impact of a single instance failure on the overall system.</li> <li>Cost-Effectiveness: Pay only for the resources you need, when you need them.</li> </ul>"},{"location":"ai-ml-api/scalability/#auto-scaling","title":"Auto-Scaling","text":"<p>Auto-scaling encompasses both horizontal scaling strategies and applies them automatically in response to traffic patterns and system load.</p> <ul> <li>Burst Scaling: Our system is capable of burst scaling, which is a form of auto-scaling designed to handle sudden spikes in traffic. This ensures that the system remains responsive during unexpected surges in demand.</li> </ul>"},{"location":"ai-ml-api/scalability/#benefits-of-auto-scaling","title":"Benefits of Auto-Scaling","text":"<ul> <li>Responsiveness: Quickly adapts to changes in load, ensuring consistent performance.</li> <li>Cost Efficiency: Resources are scaled up only when needed, reducing unnecessary expenditure.</li> </ul>"},{"location":"ai-ml-api/testing/","title":"Testing","text":"<p>We have written comprehensive unit tests for the API server, covering a wide range of scenarios and edge cases. It includes the following:</p> <ul> <li>Test Cases: Detailed test cases for each API endpoint.</li> <li>Coverage: Ensuring high test coverage to validate the functionality of the API.</li> <li>Automation: Implementing automated testing to streamline the testing process.</li> <li>Continuous Integration: Integrating testing into the CI/CD pipeline for efficient development.</li> <li>Mocking: Using mocks to simulate external dependencies and ensure isolated testing.</li> <li>Performance Testing: Conducting performance tests to evaluate the API's responsiveness and scalability.</li> </ul>"},{"location":"ai-ml-api/api-usage/authentication/","title":"Authentication","text":"<p>To obtain and use API tokens:</p> <ol> <li>Log in to the CKAN platform</li> <li>Navigate to \"Profile\" &gt; \"AI/ML API Tokens\"</li> <li>Specify token name, scopes, and expiration</li> <li>Click \"Create New Token\"</li> <li>Copy the generated token (displayed only once)</li> <li>Use the token in API requests:</li> </ol> <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer &lt;your_token_here&gt;\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"smiles\": \"c1ccccc1O\"}' \\\n  https://api.example.com/v1/ai/evaluate\n</code></pre>"},{"location":"ai-ml-api/api-usage/endpoints/","title":"Endpoints","text":"<p>This document provides comprehensive API documentation, ensuring developers have all the necessary information for successful integration.</p>"},{"location":"ai-ml-api/api-usage/endpoints/#openapi-swagger-specification","title":"OpenAPI (Swagger) Specification","text":"<p>Interactive documentation for all endpoints is available through our OpenAPI (Swagger) Specification. This allows for easy testing and exploration of the API capabilities. Access the interactive documentation here.</p>"},{"location":"ai-ml-api/api-usage/endpoints/#versioning","title":"Versioning","text":"<p>We use semantic versioning for our API to ensure backward compatibility and clear communication of changes. Each endpoint's version and deprecation schedules are documented. </p>"},{"location":"ai-ml-api/api-usage/endpoints/#detailed-error-messages","title":"Detailed Error Messages","text":"<p>Our API provides detailed error messages to help developers troubleshoot issues quickly.</p>"},{"location":"ai-ml-api/api-usage/endpoints/#health-check-endpoint","title":"Health Check Endpoint","text":"<p>A health check endpoint is available to monitor the status of the API server. This endpoint can be used to verify that the server is running and responsive.</p>"},{"location":"ai-ml-api/api-usage/error-handling/","title":"Error Handling","text":"<ul> <li>HTTP Status Codes: <ul> <li>Using appropriate status codes to indicate the outcome of API requests.</li> </ul> </li> <li>Error Messages: <ul> <li>Providing clear and informative error messages for better troubleshooting.</li> </ul> </li> <li>Error Response Format: <ul> <li>Consistent format for error responses to facilitate client-side handling.</li> </ul> </li> <li>Best Practices: <ul> <li>Guidelines for handling and troubleshooting errors effectively.</li> </ul> </li> <li>Logging: <ul> <li>Logging errors for monitoring and debugging purposes.</li> </ul> </li> <li>User-Friendly Messages: <ul> <li>Ensuring error messages are user-friendly and informative.</li> </ul> </li> <li>Documentation: <ul> <li>Documenting common errors and their resolutions for reference.</li> </ul> </li> <li>Testing: <ul> <li>Testing error scenarios to validate error handling mechanisms.</li> </ul> </li> </ul>"},{"location":"ai-ml-api/overview/data-flow/","title":"Data Flow","text":"<p>The data flow process involves several key steps, outlined below:</p> <ol> <li> <p>Client Request</p> <ul> <li>The client application sends an API request to the system. This request includes an authentication token and the necessary data or parameters for the API endpoint.</li> </ul> </li> <li> <p>Istio Service Mesh</p> <ul> <li>The request first passes through the Istio Service Mesh.</li> </ul> </li> <li> <p>Envoy Filter Interception</p> <ul> <li>The custom Envoy Filter, written in Go, intercepts the request and begins processing.</li> </ul> </li> <li> <p>Public Key Retrieval</p> <ul> <li>The Envoy Filter fetches and caches the public key from Keycloak, which will be used to verify the authentication token.</li> </ul> </li> <li> <p>Token Verification</p> <ul> <li>The Envoy Filter verifies the token's signature using the fetched public key. It also checks the token's validity, expiration date, and any other claims.</li> </ul> </li> <li> <p>Role and Permission Check</p> <ul> <li>After verifying the token, the Envoy Filter checks the user's roles and permissions encoded within the token.</li> </ul> </li> <li> <p>Revoked Token Check</p> <ul> <li>The Envoy Filter fetches and caches revoked tokens from the database, then checks if the token is revoked. If revoked, it responds with an unauthorized message to the client.</li> </ul> </li> <li> <p>Forward Valid Request</p> <ul> <li>If the token is valid and the user has the necessary roles and permissions, the Envoy Filter forwards the request to the AI/ML API Server.</li> </ul> </li> <li> <p>ML Model Interaction</p> <ul> <li>The API Server interacts with the appropriate machine learning (ML) model(s) based on the functionality requested.</li> </ul> </li> <li> <p>ML Model Processing</p> <ul> <li>The ML model(s) process the input data and generate the requested output, such as predictions, classifications, or analyses.</li> </ul> </li> <li> <p>API Response</p> <ul> <li>The API Server packages the output from the ML model(s) into an appropriate response format and sends it back to the client application through the secure communication channel.</li> </ul> </li> </ol>"},{"location":"ai-ml-api/overview/system-architecture/","title":"System Architecture","text":"<p>The AI/ML API Server is a system designed to provide a unified interface for accessing and utilizing various machine learning (ML) models developed by the consortium partners of the ALTERNATIVE project. The core component of the system is the API Server, which is built using Python, Flask, and Gunicorn.</p>"},{"location":"ai-ml-api/overview/system-architecture/#overview","title":"Overview","text":"<p>When a client application sends a request to the API Server, the request first goes through the Istio Service Mesh, which is responsible for secure communication, monitoring, and traffic management. The request is then intercepted by a custom Envoy Filter, implemented in Go, which handles token-based authentication and authorization by integrating with Keycloak, an identity and access management system.</p> <p>Keycloak manages user identities and access control policies, ensuring that only authorized users can access the API and its resources. The custom Envoy Filter validates the user's token and checks their permissions before allowing the request to proceed to the API Server.</p> <p>If the user is authenticated and authorized, the request is forwarded to the API Server, which interacts with the appropriate ML model(s) to perform the requested task, such as predictive analytics. The ML models are developed using frameworks like TensorFlow, PyTorch, R, and scikit-learn, and are provided by the ALTERNATIVE project participants.</p> <p>The API Server processes the request, obtains the necessary results from the ML model(s), and generates a response, which is then sent back to the client application through the same secure communication channel.</p> <p>The system also integrates with CKAN, a data management platform, which handles access token management. Users can create, renew, and revoke their API tokens through a user-friendly interface provided by CKAN. Revoked tokens are stored in a PostgreSQL database to ensure that they cannot be used for unauthorized access.</p> <p>The entire system is designed to be secure, scalable, and efficient. Security measures include HTTPS encryption, data encryption at rest and in transit, and role-based access control (RBAC) mechanisms. Scalability is achieved through the use of Docker for containerization and Kubernetes for container orchestration, allowing the system to scale up or down as needed to handle increased demand.</p> <p>Overall, the AI/ML API Server provides a centralized and secure way for client applications to access and utilize a variety of machine learning models, while ensuring proper authentication, authorization, and scalability.</p>"},{"location":"ai-ml-api/overview/system-architecture/#components","title":"Components","text":""},{"location":"ai-ml-api/overview/system-architecture/#aiml-api-server","title":"AI/ML API Server","text":"<ul> <li>Description: The AI/ML API server is the core component that exposes the REST API. It handles requests from clients, interfaces with machine learning models, and returns processed results. This server ensures efficient communication between the client applications and the underlying ML models.</li> <li>Responsibilities: <ul> <li>Handling incoming requests from various clients.</li> <li>Interacting with machine learning models to process data.</li> <li>Generating and returning responses based on model outputs.</li> </ul> </li> <li>Technologies: <ul> <li>Python: For implementing the server logic and interfacing with ML models.</li> <li>Flask: As the web framework for creating the RESTful API.</li> <li>Gunicorn: As the WSGI HTTP server for handling concurrent requests and ensuring high performance.</li> </ul> </li> </ul>"},{"location":"ai-ml-api/overview/system-architecture/#istio-service-mesh","title":"Istio Service Mesh","text":"<ul> <li>Description: The Istio Service Mesh is a robust infrastructure layer that provides a uniform approach to securing, connecting, and monitoring microservices. It significantly enhances the security, reliability, and observability of microservices by offering advanced features like mutual TLS authentication, traffic control, and telemetry collection.</li> <li>Responsibilities: <ul> <li>Security: Implementing strong security policies, including mutual TLS and fine-grained access controls.</li> <li>Monitoring: Collecting and analyzing telemetry data to provide deep insights into service performance and health.</li> <li>Traffic Management: Managing traffic flow between services, including load balancing, traffic splitting, and fault injection to ensure smooth and reliable service interactions.</li> </ul> </li> <li>Technologies: <ul> <li>Istio: For service mesh management, providing a comprehensive suite of features to control and observe service interactions.</li> <li>Envoy: As the high-performance proxy that intercepts and routes all traffic within the service mesh, enabling sophisticated traffic management and security enforcement.</li> </ul> </li> </ul>"},{"location":"ai-ml-api/overview/system-architecture/#custom-envoy-filter","title":"Custom Envoy Filter","text":"<ul> <li>Description: The custom Envoy filter, implemented in Go, provides robust token-based authentication and authorization. This filter seamlessly integrates with Keycloak to manage identity and access control, ensuring secure and authorized communication between services.</li> <li>Responsibilities: <ul> <li>Authentication: Verifying the identity of users or services through token validation, ensuring that only authenticated entities can access the services.</li> <li>Authorization: Enforcing access control policies by checking the permissions associated with each token, thus ensuring that only authorized actions are allowed.</li> </ul> </li> <li>Technologies: <ul> <li>Go: For developing the filter, taking advantage of its concurrency features and performance.</li> <li>Envoy: As the service proxy, which the filter extends to handle custom authentication and authorization logic.</li> <li>Keycloak: For identity and access management, providing a centralized platform for authentication and authorization services.</li> </ul> </li> </ul>"},{"location":"ai-ml-api/overview/system-architecture/#keycloak-identity-and-access-management","title":"Keycloak Identity and Access Management","text":"<ul> <li>Description: An open-source system for managing authentication, authorization, and user management services.</li> <li>Responsibilities: Identity management, access control.</li> <li>Technologies: Keycloak.</li> </ul>"},{"location":"ai-ml-api/overview/system-architecture/#ckan-data-management-platform","title":"CKAN Data Management Platform","text":"<ul> <li>Description: CKAN is a powerful data management platform designed for publishing, sharing, and accessing data. It is integrated with the API server to handle access tokens and manage user permissions, ensuring secure and efficient data access and distribution.</li> <li>Responsibilities: <ul> <li>Data Publishing: Enabling organizations to publish datasets, making them accessible to the public or specific user groups.</li> <li>Data Sharing: Facilitating the sharing of data across different users and systems, promoting data reuse and collaboration.</li> <li>Token Management: Handling the issuance, validation, and revocation of access tokens to secure API interactions.</li> <li>User Permissions Management: Managing user roles and permissions to control access to data and functionalities within the platform.</li> </ul> </li> <li>Technologies: <ul> <li>CKAN: The core technology provides a comprehensive suite of features for data management, including a web-based interface for dataset management, an API for programmatic access, and extensions for customization.</li> </ul> </li> </ul>"},{"location":"ai-ml-api/overview/system-architecture/#docker","title":"Docker","text":"<ul> <li>Description: Docker is a leading containerization platform that packages applications and their dependencies into isolated environments called containers. This ensures consistency and reliability across various deployment platforms, from development to production. It encapsulates the AI/ML API server along with its dependencies into a container, ensuring that it can run consistently across different environments. This containerization simplifies the deployment process and enhances the scalability and manageability of the server.</li> <li>Responsibilities: <ul> <li>Containerization: Encapsulating applications and their dependencies into lightweight, portable containers that can run consistently on any infrastructure.</li> <li>Deployment: Simplifying the deployment process by allowing applications to be deployed in any environment without compatibility issues, enhancing scalability and manageability.</li> </ul> </li> </ul>"},{"location":"ai-ml-api/overview/system-architecture/#kubernetes","title":"Kubernetes","text":"<ul> <li>Description: Kubernetes is a powerful container orchestration platform designed to automate the deployment, scaling, and management of containerized applications. It simplifies complex operations, ensuring applications run smoothly and efficiently across diverse environments. It manages the deployment, scaling, and lifecycle of Docker containers that host the AI/ML API server. By automating these processes, Kubernetes ensures the AI/ML API server can handle varying loads, recover from failures, and maintain high availability.</li> <li>Responsibilities: <ul> <li>Orchestration: Managing the deployment, configuration, and life cycle of containers, ensuring they operate seamlessly together.</li> <li>Scaling: Automatically adjusting the number of running containers based on demand, optimizing resource utilization and application performance.</li> <li>Self-Healing: Monitoring container health and restarting failed containers to maintain high availability.</li> <li>Load Balancing: Distributing network traffic evenly across containers to ensure reliable and efficient application performance.</li> </ul> </li> </ul>"},{"location":"ai-ml-api/overview/system-architecture/#aiml-models","title":"AI/ML Models","text":"<ul> <li>Description: The AI/ML models are diverse machine learning models contributed by participants in the ALTERNATIVE project. These models are utilized for a range of tasks, including image recognition, predictive analytics, natural language processing, and more, to provide intelligent and automated solutions.</li> <li>Responsibilities: <ul> <li>Prediction: Generating predictions based on input data, such as identifying objects in images or forecasting future trends.</li> <li>Inference: Performing real-time inference to provide instant insights and responses in various applications.</li> <li>Learning: Continuously learning and improving from new data to enhance accuracy and performance over time.</li> </ul> </li> <li>Technologies: <ul> <li>TensorFlow: A comprehensive open-source platform for building and deploying machine learning models.</li> <li>PyTorch: A flexible and efficient deep learning framework known for its dynamic computational graph and ease of use.</li> <li>scikit-learn: A robust library for classical machine learning algorithms, offering tools for data preprocessing, model training, and evaluation.</li> </ul> </li> </ul>"},{"location":"ai-ml-api/overview/system-architecture/#postgresql-database","title":"PostgreSQL Database","text":"<ul> <li>Description: PostgreSQL is a powerful, open-source relational database management system (RDBMS) used for storing and managing a wide range of data, including user information, access tokens, and other critical data. Known for its robustness and flexibility, PostgreSQL ensures data integrity and supports advanced data types and performance optimization.</li> <li>Responsibilities: <ul> <li>Data Storage: Efficiently storing and retrieving user data, access tokens, and other related information.</li> <li>Token Management: Maintaining a record of revoked tokens to ensure secure access control and prevent unauthorized access.</li> <li>Data Integrity: Enforcing constraints and transactions to ensure the accuracy and consistency of stored data.</li> </ul> </li> <li>Technologies: <ul> <li>PostgreSQL: The core technology provides advanced features such as ACID compliance, full-text search, and support for JSON and other complex data types, making it suitable for a variety of applications and data management needs.</li> </ul> </li> </ul>"},{"location":"ai-ml-api/security/authentication-and-authorization/","title":"Authentication and Authorization","text":""},{"location":"ai-ml-api/security/authentication-and-authorization/#overview","title":"Overview","text":"<p>This document outlines the authentication and authorization mechanisms implemented in our system. It covers the token-based security mechanism using an Envoy filter, integration with Keycloak for identity and access management, and the application of Role-based Access Control (RBAC) for fine-grained permissions.</p>"},{"location":"ai-ml-api/security/authentication-and-authorization/#token-based-security-mechanism","title":"Token-based Security Mechanism","text":"<p>The system employs a robust token-based security mechanism integrated within the Istio service mesh. This approach ensures that only authenticated and authorized requests can access protected services.</p>"},{"location":"ai-ml-api/security/authentication-and-authorization/#implementation-details","title":"Implementation Details","text":"<ul> <li>Envoy Filter: A custom HTTP filter deployed as a sidecar container alongside the service it secures. It performs the following functions:</li> <li>Request Interception: All incoming requests are intercepted for validation.</li> <li>Token Validation: Validates the JWT token's signature and checks the token ID against a revocation list stored in a PostgreSQL database.</li> <li>Caching: To improve performance and reduce database queries, caching is used for the token's revocation status and the public key for signature verification.</li> </ul>"},{"location":"ai-ml-api/security/authentication-and-authorization/#verification-process","title":"Verification Process","text":"<ol> <li>Extract Token: The token is extracted from the request header.</li> <li>Validate Structure and Signature: Checks if the token structure is valid and verifies the signature.</li> <li>Check Expiration: Ensures the token has not expired.</li> <li>Verify Claims: Verifies the issuer and audience claims.</li> <li>Revocation List Check: Checks the token ID against a revocation list.</li> </ol>"},{"location":"ai-ml-api/security/authentication-and-authorization/#keycloak-integration","title":"Keycloak Integration","text":"<p>Keycloak serves as the central identity and access management system, offering features like OAuth 2.0, OpenID Connect support, and multi-factor authentication.</p>"},{"location":"ai-ml-api/security/authentication-and-authorization/#integration-steps","title":"Integration Steps","text":"<ol> <li>Realm Configuration: Set up a Keycloak realm specific to the API.</li> <li>Client Application Setup: Register the client application in Keycloak.</li> <li>Roles and Permissions: Define roles and permissions for access control.</li> <li>Token Configuration: Configure token settings, including lifespan and signature algorithm.</li> <li>Library Integration: Integrate Keycloak libraries with the Envoy filter for seamless authentication and authorization.</li> </ol>"},{"location":"ai-ml-api/security/authentication-and-authorization/#access-control","title":"Access Control","text":"<p>Role-based Access Control (RBAC) is implemented to provide fine-grained access control to API endpoints. This section could be expanded with examples of role definitions and how they are applied to endpoints.</p>"},{"location":"ai-ml-api/security/authentication-and-authorization/#best-practices","title":"Best Practices","text":"<ul> <li>Regularly Update Roles: Ensure roles and permissions are regularly reviewed and updated to reflect changes in access requirements.</li> </ul>"},{"location":"ai-ml-api/security/authentication-and-authorization/#common-issues-and-troubleshooting","title":"Common Issues and Troubleshooting","text":"<ul> <li>Token Validation Failure: Ensure the token has not expired and the signature matches the public key.</li> <li>Access Denied: Verify the user's roles and permissions align with the requested resource's access control policies.</li> <li>Performance Issues: Check the caching mechanism for token validation and revocation status to ensure it is functioning correctly.</li> </ul>"},{"location":"ai-ml-api/security/encryption/","title":"Encryption","text":"<p>The implementation of the Alternative Envoy filter within an Istio service mesh is a cornerstone for ensuring robust encryption practices, thereby guaranteeing data security across the network. This document outlines the encryption strategies employed to protect data both in transit and at rest, emphasizing the importance of these measures in maintaining data integrity and confidentiality within a distributed system.</p>"},{"location":"ai-ml-api/security/encryption/#secure-data-transmission-with-https","title":"Secure Data Transmission with HTTPS","text":"<p>HTTPS is utilized as the primary means for secure data transmission. By leveraging Transport Layer Security (TLS), HTTPS provides a secure channel over which data can be transmitted between clients and services. This prevents data interception and tampering by encrypting the data during transit.</p>"},{"location":"ai-ml-api/security/encryption/#benefits-of-https","title":"Benefits of HTTPS:","text":"<ul> <li>Encryption: Ensures that data exchanged between the client and server is encrypted, protecting against eavesdropping and man-in-the-middle attacks.</li> <li>Data Integrity: Guarantees that the data sent is not altered or corrupted during transfer.</li> <li>Authentication: Verifies that the users are communicating with the intended website, building trust.</li> </ul>"},{"location":"ai-ml-api/security/encryption/#encryption-of-sensitive-data","title":"Encryption of Sensitive Data","text":"<p>Sensitive information, including JWT tokens and revocation lists, is encrypted not only in transit but also at rest. This dual-layer encryption strategy provides comprehensive protection against unauthorized access and potential data breaches.</p>"},{"location":"ai-ml-api/security/encryption/#data-in-transit","title":"Data in Transit:","text":"<ul> <li>End-to-End Encryption: For sensitive payloads, end-to-end encryption is employed, ensuring that data is encrypted from the source all the way to the destination without being decrypted at intermediary points.</li> </ul>"},{"location":"ai-ml-api/security/encryption/#encryption-algorithms-and-key-management","title":"Encryption Algorithms and Key Management","text":"<p>Choosing the right encryption algorithms and managing encryption keys effectively are critical aspects of a robust encryption strategy.</p> <ul> <li>Algorithms: AES (Advanced Encryption Standard) for data at rest and TLS 1.3 for data in transit are recommended due to their strong security features and widespread support.</li> <li>Key Management: Securely managing the keys involves generating, storing, rotating, and retiring encryption keys in a secure manner. A centralized key management system can help automate these processes, reducing the risk of human error.</li> </ul>"},{"location":"ai-ml-api/token-management/ckan-integration/","title":"Token Management with CKAN","text":""},{"location":"ai-ml-api/token-management/ckan-integration/#integration-with-ckan","title":"Integration with CKAN","text":"<p>The integration is achieved through a combination of custom development and configuration, ensuring a seamless experience for both administrators and end-users.</p>"},{"location":"ai-ml-api/token-management/ckan-integration/#custom-ckan-extension","title":"Custom CKAN Extension","text":"<p>A custom CKAN extension has been developed to bridge CKAN with Keycloak, enabling advanced token management features. This extension allows for:</p> <ul> <li>Seamless synchronization between CKAN user accounts and Keycloak authentication.</li> <li>Enhanced security measures through Keycloak's robust authentication mechanisms.</li> </ul>"},{"location":"ai-ml-api/token-management/ckan-integration/#database-schema","title":"Database Schema","text":"<p>The CKAN database schema has been extended to support the storage of token metadata, including:</p> <ul> <li>Token identifiers and associated user accounts.</li> <li>Token scopes, expiration dates, and creation timestamps.</li> </ul>"},{"location":"ai-ml-api/token-management/ckan-integration/#user-interface-enhancements","title":"User Interface Enhancements","text":"<p>To improve the user experience, we have introduced new UI components focused on token management within the CKAN platform.</p>"},{"location":"ai-ml-api/token-management/ckan-integration/#new-ui-components","title":"New UI Components","text":"<ul> <li>Token Management Dashboard: Integrated into the user profile, this dashboard provides a comprehensive overview of a user's tokens, including creation dates, scopes, and expiration dates.</li> <li>Token Creation Wizard: A user-friendly interface that guides users through the process of creating new tokens, with options to customize scopes and set expiration dates.</li> <li>Token Revocation and Renewal Interface: Easy-to-use interface that allow users to revoke or renew their tokens.</li> </ul>"},{"location":"ai-ml-api/token-management/ckan-integration/#accessibility","title":"Accessibility","text":"<p>In line with our commitment to inclusivity, the new UI components are designed to be accessible to all users, ensuring a seamless experience regardless of individual needs or preferences.</p>"},{"location":"ai-ml-api/token-management/token-lifecycle/","title":"Token Lifecycle Management","text":""},{"location":"ai-ml-api/token-management/token-lifecycle/#obtaining-tokens","title":"Obtaining Tokens","text":"<ol> <li>User initiates token creation through CKAN interface</li> <li>CKAN validates user permissions</li> <li>Token request sent to Keycloak with specified parameters</li> <li>Keycloak generates JWT with appropriate claims</li> <li>Token metadata stored in CKAN database</li> <li>Token presented to user (displayed only once for security)</li> </ol>"},{"location":"ai-ml-api/token-management/token-lifecycle/#revoking-tokens","title":"Revoking Tokens","text":"<ol> <li>The user revokes an existing access token through the CKAN platform.</li> <li>CKAN adds the revoked token to a list of invalid tokens stored in the PostgreSQL database.</li> <li>CKAN sends a request to the Envoy Filter to refresh the cache of revoked tokens.</li> </ol>"},{"location":"ai-ml-api/token-management/token-lifecycle/#expiration-policies","title":"Expiration Policies","text":"<ul> <li>Default token lifespan: 6 months (configurable)</li> </ul>"}]}