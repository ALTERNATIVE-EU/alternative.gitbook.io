{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ALTERNATIVE Platform - User Guide","text":"<p>The web platform, made with CKAN, can be used to manage, publish and find data. It also has an API, which can be used by generating an API Token; and a python library - for searching datasets and downloading resources.</p>"},{"location":"#contents","title":"Contents:","text":"<p>Users, Organizations and Authorization</p> <p>Datasets, Resources and Groups</p> <p>Metadata</p> <p>JupyterHub</p>"},{"location":"datasets/","title":"Datasets, Resources and Groups","text":"<p>Data is published in units called datasets. A dataset contains: information or metadata about the data; a number of resources, which hold the data itself. They are stored in an S3 bucket in Google Cloud Storage, or simply as a link, if the resource is elsewhere on the web.</p>"},{"location":"datasets/#exploring-datasets","title":"Exploring Datasets","text":"<p>By selecting <code>Datasets</code> you can see a list of all datasets on the platform. From the datasets page or an organization's page you can find a dataset you are interested in (more details about searching datasets). Selecting it will display the dataset page. At the top there are 3 tabs: - <code>Dataset</code> - here you can see all the information about the dataset and a list of its resources, choosing a resource will take you to its page, where you can see details about it, manage and download it - <code>Groups</code> - see any group the dataset is part of, or add it to new groups by selecting the group name and pressing <code>Add to group</code> - <code>Activity Stream</code> - see the history of changes made to the dataset</p>"},{"location":"datasets/#creating-a-new-dataset","title":"Creating a New Dataset","text":"<ol> <li>You can access the <code>Create Dataset</code> screen in two ways. Go to <code>Datasets</code> page, then select the <code>Add Dataset</code> button. Alternatively, go to <code>Organizations</code>, select the page for the organization that should own your new dataset. Provided that you are a member of this organization, you can now select the <code>Add Dataset</code> button.</li> <li>Add information about the data</li> <li>Press <code>Next: Add Data</code></li> <li>This is where you will add one or more resources which contain the data for this dataset. Choose a file (<code>Upload</code> button, max file size = 1 GB) or link (<code>Link</code> button) for your data resource. Fill the other information on the page:</li> <li>Name - a name for this resource, different resources in the dataset should have different names</li> <li>Description - a description of the resource</li> <li>Format - the file format of the resource</li> <li>If you have more resources to add to the dataset, select <code>Save &amp; add another</code>. When you have added all resources, press <code>Finish</code>.</li> </ol>"},{"location":"datasets/#managing-dataset","title":"Managing Dataset","text":"<p>You can edit the dataset you have created, or any dataset owned by an organization that you are a member of, or any dataset, in which you have been listed as a collaborator with the Editor role or higher.</p> <p>Go to the dataset\u2019s page. Select <code>Manage</code>. Here you can:</p>"},{"location":"datasets/#edit-metadata-or-delete-dataset","title":"Edit Metadata or Delete Dataset","text":"<p>In the <code>Edit metadata</code> tab you can edit any of the fields. When you have finished, press the <code>Update Dataset</code> button to save your changes. Alernatively, you can delete the dataset by pressing <code>Delete</code>. The dataset is not completely deleted. It is hidden, so it does not show up in any searches. However, by visiting the URL for the dataset\u2019s page, it can still be seen (by users with appropriate authorization), and undeleted if necessary. If it's important to completely delete the dataset - contact a sysadmin user.</p>"},{"location":"datasets/#manage-a-datasets-resources","title":"Manage a Dataset's Resources","text":"<p>In the <code>Resources</code> tab, you can add new resources to the dataset by pressing <code>Add new resource</code>, or, by selecting a resource, you can edit information about it. When you have finished editing, select the button marked <code>Update Resource</code> to save your changes. To delete the resource, press <code>Delete</code>.</p>"},{"location":"datasets/#dataset-collaborators","title":"Dataset Collaborators","text":"<p>In the <code>Collaborators</code> tab, you'll see a list of all users that have been given special permissions to the dataset (collaborators don't have to be members of the organization that owns the dataset). From the list you can <code>Edit</code> (change their role) or <code>Delete</code> a collaborator. Add new collaborator by selecting <code>Add Collaborators</code> - choose their username, assign a role and press <code>Add Collaborator</code>.</p> <p>Collaborator Roles:</p> <ul> <li>Member - can access the dataset if private, but not edit it</li> <li>Editor - can edit the dataset and its resources, as well as accessing the dataset if private</li> <li>Admin - in addition to managing the dataset, they can add and remove collaborators</li> </ul>"},{"location":"datasets/#groups","title":"Groups","text":"<p>Groups are collections of datasets (datasets in a group can be owned by different organizations). They can also have members. By selecting <code>Groups</code> near the top of any page, you can find all the existing groups. Selecting a group will take you to its page where you'll find information about it and datasets that have been added to the group. Only sysadmin users can create groups, by going to the <code>Groups</code> page and pressing <code>Add Group</code>. Initially, the group has no datasets and only 1 member - the creator.</p>"},{"location":"datasets/#management","title":"Management","text":"<p>From the group's page, select <code>Manage</code>. On this page there are 2 tabs: <code>Edit</code> - where you can change information about the group or <code>Delete</code> it; and <code>Members</code> - you can see a list of all members, add or remove users from the group, or change their role.</p> <p>Group Roles:</p> <ul> <li>Member - can add/remove datasets from the group</li> <li>Admin - do the same as a member + can edit group information, as well as manage the group's members</li> </ul>"},{"location":"introduction/","title":"Introduction","text":"<p>The Cloud Data Platform, at the heart of this project, is designed with several key objectives:</p> <ul> <li> <p>Facilitation of Data Exchange: A primary goal of the platform is to enable seamless and efficient data exchange between consortium partners throughout the duration of the ALTERNATIVE project. This feature is vital for ensuring that all participants have timely and easy access to the data necessary for the project's success.</p> </li> <li> <p>Hosting of ML In-Silico Models for Toxicity Prediction: The platform serves as a robust environment for hosting advanced Machine Learning (ML) in-silico models. These models are crucial for toxicity prediction, representing a significant stride in the field of predictive analysis and safety assessment. By providing a reliable and scalable hosting solution, the platform ensures that these models are readily accessible and operable for the project's researchers and analysts.</p> </li> <li> <p>Exposing Data and ML Services via API: Post-completion of the ALTERNATIVE project, the platform is designed to extend its utility beyond the consortium. It will expose data and ML services to external users through a well-defined Application Programming Interface (API). This extension will facilitate wider access to the project's valuable resources, thereby enhancing research and development efforts in related fields.</p> </li> </ul> <p>To achieve these ambitious goals, the Cloud Data Platform is constructed as a multi-layered software stack, each layer playing a critical role in the platform's overall functionality and performance:</p> <ul> <li> <p>Cloud and Infrastructure Layer: This foundational layer provides the necessary cloud-based infrastructure to support the platform's operations, ensuring scalability, reliability, and security.</p> </li> <li> <p>Administrative Layer: The administrative layer is responsible for overseeing the platform's overall management, including user access control, resource allocation, and monitoring of platform activities.</p> </li> <li> <p>Data Layer: At the core of the platform is the data layer, which handles the storage, processing, and management of data. This layer is optimized for high-performance data operations, essential for the effective functioning of ML models and data exchange processes.</p> </li> <li> <p>APIs Layer: The APIs layer is designed to offer a streamlined and secure interface for accessing the platform's services. It plays a crucial role in integrating the platform with external systems and in making data and ML services available to external users post-project.</p> </li> <li> <p>Microservices Layer: This layer comprises a suite of microservices, each designed to perform specific functions within the platform. The use of microservices architecture enhances the platform's modularity, flexibility, and ease of maintenance.</p> </li> </ul> <p>In summary, the Cloud Data Platform developed for the ALTERNATIVE project represents a sophisticated and multi-faceted solution, tailored to meet the project's unique requirements for data exchange, ML model hosting, and service accessibility. This document will delve into each aspect of the platform, elucidating its design, functionalities, and the value it brings to the ALTERNATIVE project and its stakeholders.</p>"},{"location":"jupyter/","title":"JupyterHub","text":"<p>The platform integrates JupyterHub as a way to interact with datasets and virtual environments. To access JupyterHub select the <code>Jupyterhub</code> button from the top right corner of any page. Everytime a user visits the JupyterHub page, a JupyterLab server is spawned for them, which enables you to work with documents and activities such as Jupyter notebooks, text editors, files and terminals. Jupyter Notebooks (.ipynb files) are documents that combine runnable code with narrative text (Markdown), equations (LaTeX), images, interactive visualizations and more. Once your server is ready, you will be redirected to it. After some time of inactivity it will be shutdown.</p>"},{"location":"jupyter/#jupyterhub_1","title":"JupyterHub","text":"<p>At the top you can find different options and settings. You can get back to JupyterHub by selecting <code>File</code> -&gt; <code>Hub Control Panel</code> or press <code>Log Out</code> to end the session. At the bottom you can see how many terminals and consoles are open and what environment is being used. On the right there are debugging tools.</p> <p>On the left there's 4 tabs: - <code>File Browser</code> - interact with files or open a <code>Launcher</code> tab from the <code>+</code> button - <code>Running Terminals and Kernels</code> - see all open tabs and running kernels and terminals, option to close or stop any/all of them - <code>Table of Contents</code> - shows information about the currently open file/notebook - <code>Extension Manager</code> - can be used to install extensions</p> <p>In the middle, you can see the currently open tabs and interact with them. From the <code>Launcher</code> tab (create new one by clicking <code>+</code>), you can create notebooks or consoles, open terminals or start new text, markdown and python files. Example of a notebook file.</p>"},{"location":"jupyter/#file-system","title":"File System","text":"<p>The <code>/home/jovyan</code> and <code>/home/shared</code> directories are not being deleted between server stops. <code>/home/jovyan</code> is personal for the user - nobody else can see its content; and has 10 GB available space. <code>/home/shared</code> is for all users - everybody can see its content; and has 20 GB available space. All other directories are deleted and recreated everytime the server is stopped/started. The folder <code>/home/jovyan/shared</code> is a symlink to <code>/home/shared</code> and can be used to interact with the files of the shared directory.</p>"},{"location":"jupyter/#environments","title":"Environments","text":"<p>Every JupyterHub server is being spawned with two environments from the start - default python environment and an additional conda environment. On <code>Launcher</code> tab under <code>Notebook</code> or <code>Console</code> you can choose which environment to use. You can also use the <code>Terminal</code>:</p>"},{"location":"jupyter/#python-virtual-environments","title":"Python Virtual Environments","text":"<p>Each environment has their own independent set of Python packages installed in their <code>site</code> directories. A virtual environment is created on top of an existing Python installation, known as the virtual environment\u2019s <code>base</code> Python, and may be isolated from the packages in the base environment, so only those explicitly installed in the virtual environment are available. Python Virtual Environment Docs.</p> <p>Make sure you create these environments in the <code>/home/jovyan</code> directory or they will get deleted.</p> <ul> <li> <p>Add New Environments: <pre><code>python -m venv /path/to/new/virtual/environment\n</code></pre></p> </li> <li> <p>Activate Different Environment: <pre><code>source environment/bin/activate\n</code></pre></p> </li> <li> <p>Install Packages: <pre><code>pip install packageName\n</code></pre></p> </li> <li> <p>Uninstall Packages: <pre><code>pip uninstall packageName\n</code></pre></p> </li> <li> <p>Show Packages In Environment: <pre><code>pip list\n</code></pre></p> </li> <li> <p>Deactivate Current Environment: <pre><code>deactivate\n</code></pre></p> </li> <li> <p>Delete Environment: <pre><code>rm -rf environment\n</code></pre></p> </li> </ul>"},{"location":"jupyter/#conda-environments","title":"Conda Environments","text":"<p>Each environment has their own independent set of Python or Conda packages installed. Conda Environments Docs. Guide - using <code>pip</code> inside conda environment.</p> <p>Make sure you create these environments in the <code>/home/jovyan</code> directory or they will get deleted.</p> <p>You can check environment names and locations with: <pre><code>bash\nconda env list\n</code></pre></p> <ul> <li> <p>Add New Environments: <pre><code>conda create --prefix /home/jovyan/Conda2\n</code></pre></p> </li> <li> <p>Activate Different Environment: <pre><code>conda activate /home/jovyan/Conda2\n</code></pre></p> </li> <li> <p>Install Packages: <pre><code>conda install pip\npip install packageName\n</code></pre></p> </li> <li> <p>Uninstall Packages: <pre><code>pip uninstall packageName\n</code></pre></p> </li> <li> <p>Show Packages In Environment: <pre><code>conda list\npip list\n</code></pre></p> </li> <li> <p>Create Kernel From Environment: <pre><code>pip install ipykernel\npython -m ipykernel install --user --name=Conda2\n</code></pre></p> </li> <li> <p>Deactivate Current Environment: <pre><code>conda deactivate\n</code></pre></p> </li> <li> <p>Delete Environment: <pre><code>conda activate base\nconda remove -p /home/jovyan/Conda2 --all\njupyter kernelspec remove conda2\n</code></pre></p> </li> </ul>"},{"location":"jupyter/#python-library","title":"Python Library","text":"<p>The python library alternative-lib is designed to help with finding datasets and downloading resources from them, by using ckanapi.</p> <p>It can be installed with: <pre><code>pip install alternative-lib\n</code></pre></p> <p>Example of the library being used to get a public dataset and download its resource.</p>"},{"location":"jupyter/#r","title":"R","text":"<p>You can use R from a <code>Terminal</code>. The first time you install a new package you should be asked where to save it. Make sure any packages you install are in the <code>/home/jovyan</code> directory or they will get deleted.</p> <ul> <li> <p>Activate R Console: <pre><code>R\n</code></pre></p> </li> <li> <p>Help Command: <pre><code>help()\n</code></pre></p> </li> <li> <p>Install Package: <pre><code>install.packages(\"package_name\")\n</code></pre></p> </li> <li> <p>List Installed Packages: <pre><code>installed.packages()\n</code></pre></p> </li> <li> <p>Exit R Console: <pre><code>q()\n</code></pre></p> </li> </ul>"},{"location":"jupyter/#useful-r-docs","title":"Useful R Docs","text":"<ul> <li>The R Project</li> <li>Kickstarting R</li> <li>Installing R Packages</li> </ul>"},{"location":"jupyter/#version-update","title":"Version Update","text":"<p>The R version has been updated from 4.0.4 to 4.3.0. If you have installed any R packages before this update, you will need to add your previous package directory to R's paths, to be able to use those packages.</p> <p>Commands (should be executed in R console; replace <code>yourLib</code> with the path to your R package directory; by default that should have been <code>~/R/x86_64-pc-linux-gnu-library/4.0</code>):</p> <p>To add the directory in paths list: <pre><code>.libPaths( c( .libPaths(), \"yourLib\") )\n</code></pre></p> <p>To make the directory the main library for packages: <pre><code>.libPaths( c( \"yourLib\" , .libPaths() ) )\n</code></pre></p>"},{"location":"jupyter/#bioconductor","title":"Bioconductor","text":"<p>Installing and Using Bioconductor. When installing packages with <code>BiocManager::install()</code>, add the path to your R package directory, like so (replace <code>your_R_package_dir</code> with the directory name): <pre><code>BiocManager::install(lib=\"/home/jovyan/your_R_package_dir\")\n</code></pre> or <pre><code>BiocManager::install(\"package_name\", lib=\"/home/jovyan/your_R_package_dir\")\n</code></pre></p>"},{"location":"metadata/","title":"Metadata","text":"<p>Each dataset has metadata associated with it. Metadata is structured reference data that helps to sort and identify the information it describes.</p>"},{"location":"metadata/#fields","title":"Fields","text":"<ul> <li>Title - this title will be unique across the platform, so make it brief but specific</li> <li>URL - automatically filled based on title, but can be edited</li> <li>Description - add a longer description of the dataset, including information such as what people need to know when using the data</li> <li>Tags - add tags that will help people find the data and link it with other related data</li> <li>License - it is important to include a license, so that people know how they can use the data</li> <li>Organization - choose which organization, that you're a member of, should own the dataset</li> <li>Visibility - a public dataset can be seen by anyone, a private one can only be seen by members of the organization owning the dataset or by collaborators of the dataset, and will not show up in searches by other users</li> <li>Source - where the data is from</li> <li>Version - version of the data</li> <li>Author - name of the person or organization responsible for producing the data</li> <li>Author e-mail - an e-mail address for the author, to which queries about the data should be sent</li> <li>Maintainer - name of the person or organization responsible for maintaining the data</li> <li>Maintainer e-mail - details for a second person responsible for the data</li> </ul>"},{"location":"metadata/#advanced-metadata-for-experiments","title":"Advanced Metadata for Experiments","text":"<p>When creating/editing a dataset, you can mark the checkbox to be able to set extra fields, related to experiment data:</p> <ul> <li>Culture medium</li> <li>Number of replicates</li> <li>Number of cells/well</li> <li>Ratio hiPSC-CMs/HCAECs</li> <li>Date of experiment</li> <li>Toxin</li> <li>Age type</li> <li>Dimension</li> <li>Category</li> <li>Content</li> <li>Model</li> </ul>"},{"location":"metadata/#custom-fields","title":"Custom Fields","text":"<p>If you want the dataset metadata to have more fields, you can add a name/key and value for it.</p>"},{"location":"metadata/#find-data","title":"Find Data","text":"<p>You should be able to find a dataset by typing the title, or some relevant words from the description/metadata, into the search box on any page, containing datasets. On the left side of the <code>Datasets</code> page there are also some filters, such as <code>Organizations</code>, <code>Groups</code>, <code>Tags</code>, <code>Formats</code>, <code>Licenses</code>. Select any number of options to restrict the search. Under the search box there's also an <code>Order by</code> field, to sort datasets in any given way. If you want to look for data owned by a particular organization/group, you can search within that from its homepage.</p> <p>CKAN uses Apache Solr as its search engine. Note that not the whole functionality is offered through the simplified search interface in CKAN or it can differ. CKAN supports two search modes, both are used from the same search field. If the search terms entered into the search field contain no colon (<code>:</code>) CKAN will perform a simple search. If the search expression does contain a colon CKAN will perform an advanced search.</p>"},{"location":"metadata/#simple-search","title":"Simple Search","text":"<p>CKAN defers most of the search to Solr and by default it uses the DisMax Query Parser that was primarily designed to be easy to use and to accept almost any input.</p> <p>The search words typed by the user in the search box defines the main query constituting the essence of the search. Some characters are treated as mandatory (<code>+</code>) and prohibited (<code>-</code>) modifiers for terms. Text wrapped in balanced quote characters (<code>\u201cexample text\u201d</code>) is treated as a phrase. By default, all words or phrases specified by the user are treated as optional unless they are preceded by <code>+</code> or <code>-</code>. CKAN will search for the complete word and when doing simple search wildcards are not supported. Solr applies some preprocessing and stemming when searching. Stemmers remove morphological affixes from words, leaving only the word stem. This may cause, for example, that searching for <code>testing</code> or <code>tested</code> will show also results containing the word <code>test</code>. If the name of the dataset contains words separated by <code>-</code> it will consider each word independently in the search.</p> <p>Examples:</p> <ul> <li><code>census</code> -&gt; will search for all the datasets containing the word <code>census</code> in the query fields</li> <li><code>census +2019</code> -&gt; will search for all the datasets contaning the word <code>census</code> and filter only those matching <code>2019</code> too</li> <li><code>census -2019</code> -&gt; will search for all the datasets containing the word <code>census</code> and will exclude the results featuring <code>2019</code></li> <li><code>\"european census\"</code> -&gt; will search for all the datasets containing the phrase <code>european census</code></li> <li><code>Testing</code> -&gt; will search for all the datasets containing the word <code>Testing</code> and also <code>Test</code> as it is the stem of <code>Testing</code></li> </ul>"},{"location":"metadata/#advanced-search","title":"Advanced Search","text":"<p>This will be considered a fielded search and the query syntax of Solr will be used to search. This will allow us to use wildcards (<code>*</code>), proximity matching (<code>~</code>, looks for terms that are within a specific distance from one another) and general features described in Solr docs. The basic syntax is <code>field:term</code>. Field names may differ from datasets' attributes, the mapping rules are defined in the schema.xml file. You can use <code>title</code> to search by the dataset name and <code>text</code> to look in a catch-all field. CKAN supports fuzzy searches based on the Levenshtein Distance, or Edit Distance algorithm, to do a fuzzy search use the <code>~</code> symbol at the end of a single-word term.</p> <p>Examples:</p> <ul> <li><code>title:european</code> -&gt; will look for all the datasets containing in their title the word <code>european</code></li> <li><code>title:europ*</code> -&gt; will look for all the datasets containing in their title a word that starts with <code>europ</code>, like <code>europe</code> and <code>european</code></li> <li><code>title:europe || title:australia</code> -&gt; will look for datasets containing <code>europe</code> or <code>australia</code> in their title</li> <li><code>title: \"european census\" ~ 4</code> -&gt; will look for datasets, in which the title contains the words <code>european</code> and <code>census</code> within a distance of 4 words</li> <li><code>author:powell~</code> -&gt; words like <code>jowell</code> or <code>pomell</code> will also be found</li> </ul>"},{"location":"networking/","title":"Networking","text":""},{"location":"networking/#overview","title":"Overview","text":"<p>The network architecture of the platform is designed to support robust and efficient operations. At its core, the network is provisioned by a cloud provider, encompassing a comprehensive suite of fundamental virtual network functions. This includes:</p> <ul> <li>Virtual Data Center (VDC)</li> <li>An array of subnets</li> <li>A combination of public and private IPs</li> <li>Network gateways</li> <li>Load balancers</li> <li>Ingress controllers</li> </ul> <p>These components collectively form the backbone of the network infrastructure, ensuring high availability and scalability.</p>"},{"location":"networking/#kubernetes-networking","title":"Kubernetes Networking","text":"<p>A central aspect of the network architecture is Kubernetes networking, as all applications are deployed on Kubernetes. This reflects a commitment to leveraging advanced container orchestration technology. The seamless integration between Kubernetes and the cloud provider is primarily due to the use of managed Kubernetes as a service. This integration not only simplifies operations but also enhances the reliability and efficiency of deployments.</p>"},{"location":"networking/#container-network-interface-cni","title":"Container Network Interface (CNI)","text":"<p>In the Kubernetes environment, Calico is selected as the Container Network Interface (CNI) plugin. Calico is known for its simplicity, scalability, and security features, making it an ideal choice for Kubernetes networking needs.</p>"},{"location":"networking/#application-networking","title":"Application Networking","text":"<p>Regarding the networking of applications, a strong emphasis is placed on the isolation of network segments. This is achieved through the use of Kubernetes PODs and namespaces, providing a high degree of control and security. Traffic ingress is efficiently managed by an NGINX-based Ingress controller, ensuring that incoming traffic is effectively routed to the appropriate services.</p>"},{"location":"networking/#security","title":"Security","text":"<p>Security is a paramount concern in the network architecture, particularly regarding data transmission. TLS encryption is employed, with cert-manager playing a crucial role in certificate management. This setup ensures the security of data in transit, providing assurance to users and stakeholders.</p>"},{"location":"networking/#dns-configuration","title":"DNS Configuration","text":"<p>DNS configuration is another key aspect of the network setup, handled by the external-dns plugin. This plugin automates the management of DNS entries, significantly reducing manual efforts and minimizing potential errors. All applications within the environment create entries in this zone, ensuring a cohesive and well-organized DNS structure.</p>"},{"location":"networking/#conclusion","title":"Conclusion","text":"<p>In summary, the network architecture is designed to not only meet the current demands of the applications but also to be adaptable and scalable for future needs, reflecting a commitment to advanced technologies and best practices.</p>"},{"location":"users/","title":"Users, Organizations and Authorization","text":"<p>There are 2 types of users: regular and sysadmin. An account is not required to search for and find data, unless the information is private, but it is needed for all publishing functions and personalization features. The platform uses Keycloak to manage user identity and access. You can find the <code>Log in</code> button from the top right corner of any page.</p>"},{"location":"users/#profile","title":"Profile","text":"<p>You can see your profile by selecting the button containing your picture and name at the top of any page. You can change the information about you, including what other users see about you by editing your profile. To do this, select the gearwheel symbol at the top of any page or go to your profile page and select <code>Manage</code>. Make the changes you want and then press <code>Update Profile</code>.</p>"},{"location":"users/#api-tokens","title":"API Tokens","text":"<p>API tokens are used in API calls or with the python library to access private datasets, that your account has rights to.</p> <p>To generate an API token:</p> <ol> <li>Go to your profile page</li> <li>Select the <code>API Tokens</code> tab</li> <li>Set a name for your token and click <code>Create API Token</code></li> <li>A green box appears above, containing your API token, make sure to copy it - you won't be able to see it again</li> </ol> <p>Underneath the creation form you can see all your tokens. From the list you can also <code>Revoke</code> a token, that you don't need anymore.</p>"},{"location":"users/#aiml-api-tokens","title":"AI/ML API Tokens","text":"<p>AI/ML API tokens are used to access the AI/ML API. To generate an AI/ML API token:</p> <ol> <li>Go to your profile page</li> <li>Select the <code>AI/ML API Tokens</code> tab</li> <li>Set a name for your token and click <code>Create AI/ML API Token</code></li> <li>Set expiration date and roles for the token</li> <li>Click <code>Create AI/ML API Token</code></li> <li>A green box appears above, containing your AI/ML API token, make sure to copy it - you won't be able to see it again</li> </ol>"},{"location":"users/#news-feed","title":"News Feed","text":"<p>At the top of any page, select the dashboard symbol. This shows changes to datasets, organizations and groups that you follow. It is possible to follow individual users to be notified of changes that they make. To start/stop following something, go to its dedicated page and select <code>Follow</code>/<code>Unfollow</code>. You can also select the <code>Activity Stream</code> tab to see all activities related to the object. You can enable email notifications for updates to things you follow by enabling <code>Subscribe to notification emails</code> from your profile settings.</p>"},{"location":"users/#organizations","title":"Organizations","text":"<p>Organizations have members and own datasets. You need to be a member of an organization in order to create datasets. Each organization can have its own workflow and authorizations, allowing it to manage its own publishing process. It also has a homepage, where users can find some information about the organization and search within its datasets, this can be accessed by going to <code>Organizations</code> and selecting the specific organization you want to explore. Only sysadmin users can create organizations, by going to the <code>Organizations</code> page and pressing <code>Add Organization</code>. Initially, the organization has no datasets and only 1 member - the creator.</p>"},{"location":"users/#management","title":"Management","text":"<p>Users with the Admin role can edit an organization's information and change the access privileges to the organization for other users. You can do so by going to the organization\u2019s page and selecting the <code>Manage</code> button. The <code>Edit</code> tab lets you change organization information or <code>Delete</code> it. From the <code>Members</code> tab you can see a list of all members, add or remove users from the organization, or change their role.</p>"},{"location":"users/#user-roles","title":"User Roles","text":"<p>A user in an organization can create a dataset owned by that organization.</p> <ul> <li>Member - can see the organization\u2019s private datasets</li> <li>Editor - anything a member can do + can edit and publish datasets</li> <li>Admin - anything an editor can do + can add, remove and change roles of organization members</li> </ul>"},{"location":"users/#mlai-roles","title":"ML/AI Roles","text":"<p>To have access to the AI/ML API models, a user needs to have the <code>ai-ml-api-{model}</code> roles. Each model has its own role, for example, to use the <code>clinicaldata</code> model, a user needs to have the <code>ai-ml-api-clinicaldata</code> role.</p>"},{"location":"ai-ml-api/","title":"AI/ML API Server","text":"<p>Welcome to the documentation for the AI/ML API Server. This server a part of the ALTERNATIVE project, providing a robust and secure interface for accessing a wide range of machine learning models. Our goal is to simplify the integration of ML models into various applications and workflows, ensuring seamless access and efficient operation.</p>"},{"location":"ai-ml-api/#overview","title":"Overview","text":"<p>The AI/ML API Server is engineered to support high-demand scenarios, offering a unified interface for machine learning models developed by consortium partners. It ensures seamless integration, secure access, and efficient operation, catering to a variety of use cases from predictive analytics to real-time data processing.</p>"},{"location":"ai-ml-api/#key-objectives","title":"Key Objectives","text":"<ul> <li>Seamless Integration: Simplify the incorporation of ML models into existing applications and workflows.</li> <li>Secure API Access: Implement state-of-the-art security measures for data protection and access control.</li> <li>Scalable Architecture: Dynamically adjust resources to handle varying loads, ensuring consistent performance.</li> <li>High Availability: Design for fault tolerance and resilience to minimize downtime.</li> <li>Comprehensive Documentation: Provide detailed guides and examples to facilitate easy adoption.</li> <li>User-Friendly Interfaces: Offer intuitive tools for managing API tokens and accessing model functionalities.</li> </ul>"},{"location":"ai-ml-api/#features","title":"Features","text":"<ul> <li>Diverse Model Support: Access a wide range of ML models for different domains and applications.</li> <li>Token-Based Authentication: Secure API endpoints with robust token authentication mechanisms.</li> <li>Scalable Deployment: Leverage Docker and Kubernetes for scalable and manageable deployments.</li> <li>Performance Monitoring: Integrated tools for tracking API performance and usage statistics.</li> <li>Interactive Documentation: Explore API functionalities with interactive Swagger documentation.</li> </ul>"},{"location":"ai-ml-api/deployment/","title":"Deployment Process","text":""},{"location":"ai-ml-api/deployment/#production-deployment","title":"Production Deployment:","text":"<p>For the production deployment, we use Kubernetes to manage the deployment of the AI/ML API server and its associated components. The deployment process involves the following steps:</p>"},{"location":"ai-ml-api/deployment/#aiml-api-server-deployment","title":"AI/ML API Server Deployment:","text":"<ol> <li>Build Docker Image</li> <li>Push Image to Registry</li> <li>Update Deployment Files</li> <li>Apply Deployment</li> </ol> <p>In the project README, we provide detailed instructions for each step of the deployment process, including the commands to execute.</p>"},{"location":"ai-ml-api/deployment/#custom-envoy-filter-deployment","title":"Custom Envoy Filter Deployment:","text":"<ol> <li>Apply the SQL Schema</li> <li>Build the Filter</li> <li>Create PVC</li> <li>Add volume and volume mount to the sidecar container</li> <li>Copy the filter binary to the volume</li> <li>Update the Envoy Filter manifest configuration</li> <li>Apply the Envoy Filter manifest</li> </ol>"},{"location":"ai-ml-api/repositories/","title":"Repositories","text":"Repository Description Link AI/ML API Core REST API component with Flask application logic GitHub Envoy Filter Custom Filter Go implementation of token-based authentication and authorization GitHub Keycloak AI/ML Access Tokens Plugin Keycloak extension for managing AI/ML API access tokens GitHub CKAN Platform Modified CKAN platform with token management features GitHub Keycloak Authorization Plugin Keycloak plugin for managing user roles and permissions GitHub"},{"location":"ai-ml-api/scalability/","title":"Scalability","text":"<p>To accommodate varying loads and optimize resource utilization, our system employs a multi-faceted approach to scalability. This document outlines the key strategies used to ensure that the system can handle growth in demand without compromising on performance.</p>"},{"location":"ai-ml-api/scalability/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>Horizontal scaling, also known as scaling out, involves adding more instances of the API servers to distribute the load evenly. This approach is particularly effective for handling an increase in user requests.</p> <ul> <li> <p>Stateless API Servers: Our API servers are designed to be stateless, which means they do not store any user data between requests. This design choice allows us to add or remove server instances without impacting the system's state or performance.</p> </li> <li> <p>Kubernetes Horizontal Node Scaling: We leverage on the Cloud Provider's Kubernetes environment to automatically scale the number of nodes based on resource utilization. This ensures that the system can handle varying loads efficiently.</p> </li> </ul>"},{"location":"ai-ml-api/scalability/#benefits-of-horizontal-scaling","title":"Benefits of Horizontal Scaling","text":"<ul> <li>Flexibility: Easily adjust capacity by adding or removing instances.</li> <li>Fault Tolerance: Reduced impact of a single instance failure on the overall system.</li> <li>Cost-Effectiveness: Pay only for the resources you need, when you need them.</li> </ul>"},{"location":"ai-ml-api/scalability/#auto-scaling","title":"Auto-Scaling","text":"<p>Auto-scaling encompasses both horizontal scaling strategies and applies them automatically in response to traffic patterns and system load.</p> <ul> <li>Burst Scaling: Our system is capable of burst scaling, which is a form of auto-scaling designed to handle sudden spikes in traffic. This ensures that the system remains responsive during unexpected surges in demand.</li> </ul>"},{"location":"ai-ml-api/scalability/#benefits-of-auto-scaling","title":"Benefits of Auto-Scaling","text":"<ul> <li>Responsiveness: Quickly adapts to changes in load, ensuring consistent performance.</li> <li>Cost Efficiency: Resources are scaled up only when needed, reducing unnecessary expenditure.</li> </ul>"},{"location":"ai-ml-api/testing/","title":"Testing","text":"<p>We have written comprehensive unit tests for the API server, covering a wide range of scenarios and edge cases. It includes the following:</p> <ul> <li>Test Cases: Detailed test cases for each API endpoint.</li> <li>Coverage: Ensuring high test coverage to validate the functionality of the API.</li> <li>Automation: Implementing automated testing to streamline the testing process.</li> <li>Continuous Integration: Integrating testing into the CI/CD pipeline for efficient development.</li> <li>Mocking: Using mocks to simulate external dependencies and ensure isolated testing.</li> <li>Performance Testing: Conducting performance tests to evaluate the API's responsiveness and scalability.</li> </ul>"},{"location":"ai-ml-api/api-usage/authentication/","title":"Authentication","text":"<p>To obtain and use API tokens:</p> <ol> <li>Log in to the CKAN platform</li> <li>Navigate to \"Profile\" &gt; \"AI/ML API Tokens\"</li> <li>Specify token name, scopes, and expiration</li> <li>Click \"Create New Token\"</li> <li>Copy the generated token (displayed only once)</li> <li>Use the token in API requests:</li> </ol> <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer &lt;your_token_here&gt;\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"smiles\": \"c1ccccc1O\"}' \\\n  https://api.example.com/v1/ai/evaluate\n</code></pre>"},{"location":"ai-ml-api/api-usage/endpoints/","title":"Endpoints","text":"<p>This document provides comprehensive API documentation, ensuring developers have all the necessary information for successful integration.</p>"},{"location":"ai-ml-api/api-usage/endpoints/#openapi-swagger-specification","title":"OpenAPI (Swagger) Specification","text":"<p>Interactive documentation for all endpoints is available through our OpenAPI (Swagger) Specification. This allows for easy testing and exploration of the API capabilities. Access the interactive documentation here.</p>"},{"location":"ai-ml-api/api-usage/endpoints/#versioning","title":"Versioning","text":"<p>We use semantic versioning for our API to ensure backward compatibility and clear communication of changes. Each endpoint's version and deprecation schedules are documented. </p>"},{"location":"ai-ml-api/api-usage/endpoints/#detailed-error-messages","title":"Detailed Error Messages","text":"<p>Our API provides detailed error messages to help developers troubleshoot issues quickly.</p>"},{"location":"ai-ml-api/api-usage/endpoints/#health-check-endpoint","title":"Health Check Endpoint","text":"<p>A health check endpoint is available to monitor the status of the API server. This endpoint can be used to verify that the server is running and responsive.</p>"},{"location":"ai-ml-api/api-usage/error-handling/","title":"Error Handling","text":"<ul> <li>HTTP Status Codes: <ul> <li>Using appropriate status codes to indicate the outcome of API requests.</li> </ul> </li> <li>Error Messages: <ul> <li>Providing clear and informative error messages for better troubleshooting.</li> </ul> </li> <li>Error Response Format: <ul> <li>Consistent format for error responses to facilitate client-side handling.</li> </ul> </li> <li>Best Practices: <ul> <li>Guidelines for handling and troubleshooting errors effectively.</li> </ul> </li> <li>Logging: <ul> <li>Logging errors for monitoring and debugging purposes.</li> </ul> </li> <li>User-Friendly Messages: <ul> <li>Ensuring error messages are user-friendly and informative.</li> </ul> </li> <li>Documentation: <ul> <li>Documenting common errors and their resolutions for reference.</li> </ul> </li> <li>Testing: <ul> <li>Testing error scenarios to validate error handling mechanisms.</li> </ul> </li> </ul>"},{"location":"ai-ml-api/overview/data-flow/","title":"Data Flow","text":"<p>The data flow process involves several key steps, outlined below:</p> <ol> <li> <p>Client Request</p> <ul> <li>The client application sends an API request to the system. This request includes an authentication token and the necessary data or parameters for the API endpoint.</li> </ul> </li> <li> <p>Istio Service Mesh</p> <ul> <li>The request first passes through the Istio Service Mesh.</li> </ul> </li> <li> <p>Envoy Filter Interception</p> <ul> <li>The custom Envoy Filter, written in Go, intercepts the request and begins processing.</li> </ul> </li> <li> <p>Public Key Retrieval</p> <ul> <li>The Envoy Filter fetches and caches the public key from Keycloak, which will be used to verify the authentication token.</li> </ul> </li> <li> <p>Token Verification</p> <ul> <li>The Envoy Filter verifies the token's signature using the fetched public key. It also checks the token's validity, expiration date, and any other claims.</li> </ul> </li> <li> <p>Role and Permission Check</p> <ul> <li>After verifying the token, the Envoy Filter checks the user's roles and permissions encoded within the token.</li> </ul> </li> <li> <p>Revoked Token Check</p> <ul> <li>The Envoy Filter fetches and caches revoked tokens from the database, then checks if the token is revoked. If revoked, it responds with an unauthorized message to the client.</li> </ul> </li> <li> <p>Forward Valid Request</p> <ul> <li>If the token is valid and the user has the necessary roles and permissions, the Envoy Filter forwards the request to the AI/ML API Server.</li> </ul> </li> <li> <p>ML Model Interaction</p> <ul> <li>The API Server interacts with the appropriate machine learning (ML) model(s) based on the functionality requested.</li> </ul> </li> <li> <p>ML Model Processing</p> <ul> <li>The ML model(s) process the input data and generate the requested output, such as predictions, classifications, or analyses.</li> </ul> </li> <li> <p>API Response</p> <ul> <li>The API Server packages the output from the ML model(s) into an appropriate response format and sends it back to the client application through the secure communication channel.</li> </ul> </li> </ol>"},{"location":"ai-ml-api/overview/system-architecture/","title":"System Architecture","text":"<p>The AI/ML API Server is a system designed to provide a unified interface for accessing and utilizing various machine learning (ML) models developed by the consortium partners of the ALTERNATIVE project. The core component of the system is the API Server, which is built using Python, Flask, and Gunicorn.</p>"},{"location":"ai-ml-api/overview/system-architecture/#overview","title":"Overview","text":"<p>When a client application sends a request to the API Server, the request first goes through the Istio Service Mesh, which is responsible for secure communication, monitoring, and traffic management. The request is then intercepted by a custom Envoy Filter, implemented in Go, which handles token-based authentication and authorization by integrating with Keycloak, an identity and access management system.</p> <p>Keycloak manages user identities and access control policies, ensuring that only authorized users can access the API and its resources. The custom Envoy Filter validates the user's token and checks their permissions before allowing the request to proceed to the API Server.</p> <p>If the user is authenticated and authorized, the request is forwarded to the API Server, which interacts with the appropriate ML model(s) to perform the requested task, such as predictive analytics. The ML models are developed using frameworks like TensorFlow, PyTorch, R, and scikit-learn, and are provided by the ALTERNATIVE project participants.</p> <p>The API Server processes the request, obtains the necessary results from the ML model(s), and generates a response, which is then sent back to the client application through the same secure communication channel.</p> <p>The system also integrates with CKAN, a data management platform, which handles access token management. Users can create, renew, and revoke their API tokens through a user-friendly interface provided by CKAN. Revoked tokens are stored in a PostgreSQL database to ensure that they cannot be used for unauthorized access.</p> <p>The entire system is designed to be secure, scalable, and efficient. Security measures include HTTPS encryption, data encryption at rest and in transit, and role-based access control (RBAC) mechanisms. Scalability is achieved through the use of Docker for containerization and Kubernetes for container orchestration, allowing the system to scale up or down as needed to handle increased demand.</p> <p>Overall, the AI/ML API Server provides a centralized and secure way for client applications to access and utilize a variety of machine learning models, while ensuring proper authentication, authorization, and scalability.</p>"},{"location":"ai-ml-api/overview/system-architecture/#components","title":"Components","text":""},{"location":"ai-ml-api/overview/system-architecture/#aiml-api-server","title":"AI/ML API Server","text":"<ul> <li>Description: The AI/ML API server is the core component that exposes the REST API. It handles requests from clients, interfaces with machine learning models, and returns processed results. This server ensures efficient communication between the client applications and the underlying ML models.</li> <li>Responsibilities: <ul> <li>Handling incoming requests from various clients.</li> <li>Interacting with machine learning models to process data.</li> <li>Generating and returning responses based on model outputs.</li> </ul> </li> <li>Technologies: <ul> <li>Python: For implementing the server logic and interfacing with ML models.</li> <li>Flask: As the web framework for creating the RESTful API.</li> <li>Gunicorn: As the WSGI HTTP server for handling concurrent requests and ensuring high performance.</li> </ul> </li> </ul>"},{"location":"ai-ml-api/overview/system-architecture/#istio-service-mesh","title":"Istio Service Mesh","text":"<ul> <li>Description: The Istio Service Mesh is a robust infrastructure layer that provides a uniform approach to securing, connecting, and monitoring microservices. It significantly enhances the security, reliability, and observability of microservices by offering advanced features like mutual TLS authentication, traffic control, and telemetry collection.</li> <li>Responsibilities: <ul> <li>Security: Implementing strong security policies, including mutual TLS and fine-grained access controls.</li> <li>Monitoring: Collecting and analyzing telemetry data to provide deep insights into service performance and health.</li> <li>Traffic Management: Managing traffic flow between services, including load balancing, traffic splitting, and fault injection to ensure smooth and reliable service interactions.</li> </ul> </li> <li>Technologies: <ul> <li>Istio: For service mesh management, providing a comprehensive suite of features to control and observe service interactions.</li> <li>Envoy: As the high-performance proxy that intercepts and routes all traffic within the service mesh, enabling sophisticated traffic management and security enforcement.</li> </ul> </li> </ul>"},{"location":"ai-ml-api/overview/system-architecture/#custom-envoy-filter","title":"Custom Envoy Filter","text":"<ul> <li>Description: The custom Envoy filter, implemented in Go, provides robust token-based authentication and authorization. This filter seamlessly integrates with Keycloak to manage identity and access control, ensuring secure and authorized communication between services.</li> <li>Responsibilities: <ul> <li>Authentication: Verifying the identity of users or services through token validation, ensuring that only authenticated entities can access the services.</li> <li>Authorization: Enforcing access control policies by checking the permissions associated with each token, thus ensuring that only authorized actions are allowed.</li> </ul> </li> <li>Technologies: <ul> <li>Go: For developing the filter, taking advantage of its concurrency features and performance.</li> <li>Envoy: As the service proxy, which the filter extends to handle custom authentication and authorization logic.</li> <li>Keycloak: For identity and access management, providing a centralized platform for authentication and authorization services.</li> </ul> </li> </ul>"},{"location":"ai-ml-api/overview/system-architecture/#keycloak-identity-and-access-management","title":"Keycloak Identity and Access Management","text":"<ul> <li>Description: An open-source system for managing authentication, authorization, and user management services.</li> <li>Responsibilities: Identity management, access control.</li> <li>Technologies: Keycloak.</li> </ul>"},{"location":"ai-ml-api/overview/system-architecture/#ckan-data-management-platform","title":"CKAN Data Management Platform","text":"<ul> <li>Description: CKAN is a powerful data management platform designed for publishing, sharing, and accessing data. It is integrated with the API server to handle access tokens and manage user permissions, ensuring secure and efficient data access and distribution.</li> <li>Responsibilities: <ul> <li>Data Publishing: Enabling organizations to publish datasets, making them accessible to the public or specific user groups.</li> <li>Data Sharing: Facilitating the sharing of data across different users and systems, promoting data reuse and collaboration.</li> <li>Token Management: Handling the issuance, validation, and revocation of access tokens to secure API interactions.</li> <li>User Permissions Management: Managing user roles and permissions to control access to data and functionalities within the platform.</li> </ul> </li> <li>Technologies: <ul> <li>CKAN: The core technology provides a comprehensive suite of features for data management, including a web-based interface for dataset management, an API for programmatic access, and extensions for customization.</li> </ul> </li> </ul>"},{"location":"ai-ml-api/overview/system-architecture/#docker","title":"Docker","text":"<ul> <li>Description: Docker is a leading containerization platform that packages applications and their dependencies into isolated environments called containers. This ensures consistency and reliability across various deployment platforms, from development to production. It encapsulates the AI/ML API server along with its dependencies into a container, ensuring that it can run consistently across different environments. This containerization simplifies the deployment process and enhances the scalability and manageability of the server.</li> <li>Responsibilities: <ul> <li>Containerization: Encapsulating applications and their dependencies into lightweight, portable containers that can run consistently on any infrastructure.</li> <li>Deployment: Simplifying the deployment process by allowing applications to be deployed in any environment without compatibility issues, enhancing scalability and manageability.</li> </ul> </li> </ul>"},{"location":"ai-ml-api/overview/system-architecture/#kubernetes","title":"Kubernetes","text":"<ul> <li>Description: Kubernetes is a powerful container orchestration platform designed to automate the deployment, scaling, and management of containerized applications. It simplifies complex operations, ensuring applications run smoothly and efficiently across diverse environments. It manages the deployment, scaling, and lifecycle of Docker containers that host the AI/ML API server. By automating these processes, Kubernetes ensures the AI/ML API server can handle varying loads, recover from failures, and maintain high availability.</li> <li>Responsibilities: <ul> <li>Orchestration: Managing the deployment, configuration, and life cycle of containers, ensuring they operate seamlessly together.</li> <li>Scaling: Automatically adjusting the number of running containers based on demand, optimizing resource utilization and application performance.</li> <li>Self-Healing: Monitoring container health and restarting failed containers to maintain high availability.</li> <li>Load Balancing: Distributing network traffic evenly across containers to ensure reliable and efficient application performance.</li> </ul> </li> </ul>"},{"location":"ai-ml-api/overview/system-architecture/#aiml-models","title":"AI/ML Models","text":"<ul> <li>Description: The AI/ML models are diverse machine learning models contributed by participants in the ALTERNATIVE project. These models are utilized for a range of tasks, including image recognition, predictive analytics, natural language processing, and more, to provide intelligent and automated solutions.</li> <li>Responsibilities: <ul> <li>Prediction: Generating predictions based on input data, such as identifying objects in images or forecasting future trends.</li> <li>Inference: Performing real-time inference to provide instant insights and responses in various applications.</li> <li>Learning: Continuously learning and improving from new data to enhance accuracy and performance over time.</li> </ul> </li> <li>Technologies: <ul> <li>TensorFlow: A comprehensive open-source platform for building and deploying machine learning models.</li> <li>PyTorch: A flexible and efficient deep learning framework known for its dynamic computational graph and ease of use.</li> <li>scikit-learn: A robust library for classical machine learning algorithms, offering tools for data preprocessing, model training, and evaluation.</li> </ul> </li> </ul>"},{"location":"ai-ml-api/overview/system-architecture/#postgresql-database","title":"PostgreSQL Database","text":"<ul> <li>Description: PostgreSQL is a powerful, open-source relational database management system (RDBMS) used for storing and managing a wide range of data, including user information, access tokens, and other critical data. Known for its robustness and flexibility, PostgreSQL ensures data integrity and supports advanced data types and performance optimization.</li> <li>Responsibilities: <ul> <li>Data Storage: Efficiently storing and retrieving user data, access tokens, and other related information.</li> <li>Token Management: Maintaining a record of revoked tokens to ensure secure access control and prevent unauthorized access.</li> <li>Data Integrity: Enforcing constraints and transactions to ensure the accuracy and consistency of stored data.</li> </ul> </li> <li>Technologies: <ul> <li>PostgreSQL: The core technology provides advanced features such as ACID compliance, full-text search, and support for JSON and other complex data types, making it suitable for a variety of applications and data management needs.</li> </ul> </li> </ul>"},{"location":"ai-ml-api/security/authentication-and-authorization/","title":"Authentication and Authorization","text":""},{"location":"ai-ml-api/security/authentication-and-authorization/#overview","title":"Overview","text":"<p>This document outlines the authentication and authorization mechanisms implemented in our system. It covers the token-based security mechanism using an Envoy filter, integration with Keycloak for identity and access management, and the application of Role-based Access Control (RBAC) for fine-grained permissions.</p>"},{"location":"ai-ml-api/security/authentication-and-authorization/#token-based-security-mechanism","title":"Token-based Security Mechanism","text":"<p>The system employs a robust token-based security mechanism integrated within the Istio service mesh. This approach ensures that only authenticated and authorized requests can access protected services.</p>"},{"location":"ai-ml-api/security/authentication-and-authorization/#implementation-details","title":"Implementation Details","text":"<ul> <li>Envoy Filter: A custom HTTP filter deployed as a sidecar container alongside the service it secures. It performs the following functions:</li> <li>Request Interception: All incoming requests are intercepted for validation.</li> <li>Token Validation: Validates the JWT token's signature and checks the token ID against a revocation list stored in a PostgreSQL database.</li> <li>Caching: To improve performance and reduce database queries, caching is used for the token's revocation status and the public key for signature verification.</li> </ul>"},{"location":"ai-ml-api/security/authentication-and-authorization/#verification-process","title":"Verification Process","text":"<ol> <li>Extract Token: The token is extracted from the request header.</li> <li>Validate Structure and Signature: Checks if the token structure is valid and verifies the signature.</li> <li>Check Expiration: Ensures the token has not expired.</li> <li>Verify Claims: Verifies the issuer and audience claims.</li> <li>Revocation List Check: Checks the token ID against a revocation list.</li> </ol>"},{"location":"ai-ml-api/security/authentication-and-authorization/#keycloak-integration","title":"Keycloak Integration","text":"<p>Keycloak serves as the central identity and access management system, offering features like OAuth 2.0, OpenID Connect support, and multi-factor authentication.</p>"},{"location":"ai-ml-api/security/authentication-and-authorization/#integration-steps","title":"Integration Steps","text":"<ol> <li>Realm Configuration: Set up a Keycloak realm specific to the API.</li> <li>Client Application Setup: Register the client application in Keycloak.</li> <li>Roles and Permissions: Define roles and permissions for access control.</li> <li>Token Configuration: Configure token settings, including lifespan and signature algorithm.</li> <li>Library Integration: Integrate Keycloak libraries with the Envoy filter for seamless authentication and authorization.</li> </ol>"},{"location":"ai-ml-api/security/authentication-and-authorization/#access-control","title":"Access Control","text":"<p>Role-based Access Control (RBAC) is implemented to provide fine-grained access control to API endpoints. This section could be expanded with examples of role definitions and how they are applied to endpoints.</p>"},{"location":"ai-ml-api/security/authentication-and-authorization/#best-practices","title":"Best Practices","text":"<ul> <li>Regularly Update Roles: Ensure roles and permissions are regularly reviewed and updated to reflect changes in access requirements.</li> </ul>"},{"location":"ai-ml-api/security/authentication-and-authorization/#common-issues-and-troubleshooting","title":"Common Issues and Troubleshooting","text":"<ul> <li>Token Validation Failure: Ensure the token has not expired and the signature matches the public key.</li> <li>Access Denied: Verify the user's roles and permissions align with the requested resource's access control policies.</li> <li>Performance Issues: Check the caching mechanism for token validation and revocation status to ensure it is functioning correctly.</li> </ul>"},{"location":"ai-ml-api/security/encryption/","title":"Encryption","text":"<p>The implementation of the Alternative Envoy filter within an Istio service mesh is a cornerstone for ensuring robust encryption practices, thereby guaranteeing data security across the network. This document outlines the encryption strategies employed to protect data both in transit and at rest, emphasizing the importance of these measures in maintaining data integrity and confidentiality within a distributed system.</p>"},{"location":"ai-ml-api/security/encryption/#secure-data-transmission-with-https","title":"Secure Data Transmission with HTTPS","text":"<p>HTTPS is utilized as the primary means for secure data transmission. By leveraging Transport Layer Security (TLS), HTTPS provides a secure channel over which data can be transmitted between clients and services. This prevents data interception and tampering by encrypting the data during transit.</p>"},{"location":"ai-ml-api/security/encryption/#benefits-of-https","title":"Benefits of HTTPS:","text":"<ul> <li>Encryption: Ensures that data exchanged between the client and server is encrypted, protecting against eavesdropping and man-in-the-middle attacks.</li> <li>Data Integrity: Guarantees that the data sent is not altered or corrupted during transfer.</li> <li>Authentication: Verifies that the users are communicating with the intended website, building trust.</li> </ul>"},{"location":"ai-ml-api/security/encryption/#encryption-of-sensitive-data","title":"Encryption of Sensitive Data","text":"<p>Sensitive information, including JWT tokens and revocation lists, is encrypted not only in transit but also at rest. This dual-layer encryption strategy provides comprehensive protection against unauthorized access and potential data breaches.</p>"},{"location":"ai-ml-api/security/encryption/#data-in-transit","title":"Data in Transit:","text":"<ul> <li>End-to-End Encryption: For sensitive payloads, end-to-end encryption is employed, ensuring that data is encrypted from the source all the way to the destination without being decrypted at intermediary points.</li> </ul>"},{"location":"ai-ml-api/security/encryption/#encryption-algorithms-and-key-management","title":"Encryption Algorithms and Key Management","text":"<p>Choosing the right encryption algorithms and managing encryption keys effectively are critical aspects of a robust encryption strategy.</p> <ul> <li>Algorithms: AES (Advanced Encryption Standard) for data at rest and TLS 1.3 for data in transit are recommended due to their strong security features and widespread support.</li> <li>Key Management: Securely managing the keys involves generating, storing, rotating, and retiring encryption keys in a secure manner. A centralized key management system can help automate these processes, reducing the risk of human error.</li> </ul>"},{"location":"ai-ml-api/token-management/ckan-integration/","title":"Token Management with CKAN","text":""},{"location":"ai-ml-api/token-management/ckan-integration/#integration-with-ckan","title":"Integration with CKAN","text":"<p>The integration is achieved through a combination of custom development and configuration, ensuring a seamless experience for both administrators and end-users.</p>"},{"location":"ai-ml-api/token-management/ckan-integration/#custom-ckan-extension","title":"Custom CKAN Extension","text":"<p>A custom CKAN extension has been developed to bridge CKAN with Keycloak, enabling advanced token management features. This extension allows for:</p> <ul> <li>Seamless synchronization between CKAN user accounts and Keycloak authentication.</li> <li>Enhanced security measures through Keycloak's robust authentication mechanisms.</li> </ul>"},{"location":"ai-ml-api/token-management/ckan-integration/#database-schema","title":"Database Schema","text":"<p>The CKAN database schema has been extended to support the storage of token metadata, including:</p> <ul> <li>Token identifiers and associated user accounts.</li> <li>Token scopes, expiration dates, and creation timestamps.</li> </ul>"},{"location":"ai-ml-api/token-management/ckan-integration/#user-interface-enhancements","title":"User Interface Enhancements","text":"<p>To improve the user experience, we have introduced new UI components focused on token management within the CKAN platform.</p>"},{"location":"ai-ml-api/token-management/ckan-integration/#new-ui-components","title":"New UI Components","text":"<ul> <li>Token Management Dashboard: Integrated into the user profile, this dashboard provides a comprehensive overview of a user's tokens, including creation dates, scopes, and expiration dates.</li> <li>Token Creation Wizard: A user-friendly interface that guides users through the process of creating new tokens, with options to customize scopes and set expiration dates.</li> <li>Token Revocation and Renewal Interface: Easy-to-use interface that allow users to revoke or renew their tokens.</li> </ul>"},{"location":"ai-ml-api/token-management/ckan-integration/#accessibility","title":"Accessibility","text":"<p>In line with our commitment to inclusivity, the new UI components are designed to be accessible to all users, ensuring a seamless experience regardless of individual needs or preferences.</p>"},{"location":"ai-ml-api/token-management/token-lifecycle/","title":"Token Lifecycle Management","text":""},{"location":"ai-ml-api/token-management/token-lifecycle/#obtaining-tokens","title":"Obtaining Tokens","text":"<ol> <li>User initiates token creation through CKAN interface</li> <li>CKAN validates user permissions</li> <li>Token request sent to Keycloak with specified parameters</li> <li>Keycloak generates JWT with appropriate claims</li> <li>Token metadata stored in CKAN database</li> <li>Token presented to user (displayed only once for security)</li> </ol>"},{"location":"ai-ml-api/token-management/token-lifecycle/#revoking-tokens","title":"Revoking Tokens","text":"<ol> <li>The user revokes an existing access token through the CKAN platform.</li> <li>CKAN adds the revoked token to a list of invalid tokens stored in the PostgreSQL database.</li> <li>CKAN sends a request to the Envoy Filter to refresh the cache of revoked tokens.</li> </ol>"},{"location":"ai-ml-api/token-management/token-lifecycle/#expiration-policies","title":"Expiration Policies","text":"<ul> <li>Default token lifespan: 6 months (configurable)</li> </ul>"},{"location":"building-blocks/administrative-services/","title":"Administrative Services","text":"<p>This section details the administrative services integral to the ALTERNATIVE platform, focusing on user credential management, DNS management, certificate lifecycle management, and ingress traffic handling. These services are essential for ensuring platform security, accessibility, and efficient operation.</p>"},{"location":"building-blocks/administrative-services/#keycloak","title":"Keycloak","text":"<p>Keycloak, established as an industry-recognized solution for security, is deployed as the primary user credential management service within the ALTERNATIVE platform. It offers robust identity and access management (IAM) capabilities, which are crucial for securing sensitive data and resources. Keycloak's integration across the platform facilitates Single Sign-On (SSO) across all user-facing applications, significantly streamlining the authentication process while upholding stringent security standards.</p> <p>A key feature of Keycloak is its ability to provide fine-grained access control, which is integrated tightly into various end-user applications. This feature allows for a more nuanced and tailored approach to access management, ensuring that users have the appropriate level of access for their roles. Furthermore, Keycloak supports advanced security features such as Two-Factor Authentication (2FA), adding an additional layer of security and significantly bolstering the platform's defense against unauthorized access.</p> <p>Keycloak's use of the OpenID Connect (OIDC) protocol, which is based on OAuth 2.0, further underscores its robustness and adaptability in the realm of secure user authentication and authorization. This protocol not only enhances the security of the platform but also ensures compatibility and ease of integration with a wide range of applications and services. The combination of these advanced features positions Keycloak as a comprehensive and secure solution for managing user credentials and access within the ALTERNATIVE platform.</p>"},{"location":"building-blocks/administrative-services/#external-dns","title":"External-DNS","text":"<p>The External-DNS service plays a pivotal role in the ALTERNATIVE platform by automating the management of DNS records for the platform's applications and services. This dynamic service is designed to respond to changes within the platform's environment, ensuring that DNS entries are consistently accurate and current. Such automation is crucial in a cloud-native environment, where service endpoints and their corresponding IP addresses are often allocated dynamically and can change frequently. This feature of external-dns significantly aids in maintaining the discoverability and accessibility of services, a key aspect of efficient cloud operations.</p> <p>An essential function of the DNS server within this context is to map human-readable URLs to these dynamically allocated IP addresses. This mapping is not only vital for user convenience and the seamless operation of the platform but also plays a critical role in the platform's security infrastructure. Specifically, the DNS entries managed by external-dns are a prerequisite for the issuance of TLS certificates by the Cert-manager. These certificates, essential for encrypting data and securing communications, rely on accurate DNS records to validate the authenticity of the platform's services.</p> <p>By integrating the external-dns service with the platform's DNS server and cert-manager, the ALTERNATIVE platform ensures a cohesive and secure environment. This integration allows for the seamless issuance and management of TLS certificates, bolstering the platform's overall security posture while ensuring that services remain easily accessible and identifiable to users. The combination of these technologies exemplifies a well-architected cloud environment, prioritizing both operational efficiency and stringent security measures.</p>"},{"location":"building-blocks/administrative-services/#cert-manager","title":"Cert-manager","text":"<p>Cert-manager is used for managing the lifecycle of SSL/TLS certificates for the platform's applications and services. It automates certificate issuance, renewal, and management processes by interfacing with Let's Encrypt, a certificate authority provider. This service is essential for ensuring encrypted and secure communication channels across the platform, thereby enhancing data security and integrity.</p>"},{"location":"building-blocks/administrative-services/#ingress-controller","title":"Ingress Controller","text":"<p>The platform employs an NGINX-based ingress controller to manage and route ingress traffic to the appropriate internal services. This controller acts as a reverse proxy and load balancer, handling incoming HTTP/S requests and directing them based on URL paths and other criteria. The ingress controller is a critical component for managing external access to the platform's services, providing a secure and efficient gateway for incoming traffic.</p> <p>Each of these administrative services plays a vital role in the overall functionality and security of the ALTERNATIVE platform. They are configured and maintained to ensure optimal performance, security compliance, and seamless integration with other platform components. All of the administrative services are deployed and run on top of Kubernetes.</p>"},{"location":"building-blocks/administrative-services/#istio-service-mesh","title":"Istio Service Mesh","text":"<p>Istio is a service mesh that provides a comprehensive solution for managing microservices within the ALTERNATIVE platform. It offers advanced features such as traffic management, security, and observability, enhancing the platform's operational capabilities and security posture.</p> <p>The platform leverages Istio's envoy filters to implement custom security policies, such as access control, and authentication. These policies help protect the platform from various security threats and ensure that only authorized traffic is allowed to access the services.</p>"},{"location":"building-blocks/apis/","title":"CKAN API","text":"<p>CKAN exposes an API which is more thoroughly documented here. The API exposes most of CKAN\u2019s features and is the preferred way to work with datasets. Jupyter users who want to pull data from CKAN can use this API directly or use Python\u2019s <code>alternative-lib</code> library.</p>"},{"location":"building-blocks/cloud-infrastructure/","title":"Cloud Infrastructure","text":"<p>The cloud infrastructure forms the foundational layer of the ALTERNATIVE data platform, consisting of virtual services and hardware components. This infrastructure is specifically architected to underpin the platform's diverse and demanding operational needs.</p>"},{"location":"building-blocks/cloud-infrastructure/#storage-solutions","title":"Storage Solutions","text":"<p>At the heart of this infrastructure lies a suite of storage solutions, including both block storage and object storage services (such as S3). These storage services are designed to offer high durability, scalability, and accessibility, ensuring that data is securely stored and readily available when needed.</p>"},{"location":"building-blocks/cloud-infrastructure/#computational-resources","title":"Computational Resources","text":"<p>In terms of computational resources, the infrastructure boasts an array of Virtual Machines, each equipped with scalable CPU and Memory capabilities. Those are usually deployed by the Kubernetes managed service and act as Kubernetes nodes rather than serving as VMs directly.</p>"},{"location":"building-blocks/cloud-infrastructure/#virtual-networking","title":"Virtual Networking","text":"<p>A key component of the cloud infrastructure is the virtual networking aspect. This includes a comprehensive network architecture that ensures secure, fast, and reliable communication between various services and components within the platform. The virtual networking setup is integral to maintaining the overall performance and integrity of the cloud environment.</p>"},{"location":"building-blocks/cloud-infrastructure/#kubernetes-cluster","title":"Kubernetes Cluster","text":"<p>Central to the infrastructure's capabilities is the Kubernetes cluster. This cluster plays a pivotal role in orchestrating containerized applications, ensuring their seamless deployment, scaling, and management. Kubernetes, renowned for its efficiency and flexibility, enhances the platform's ability to support a wide range of applications and services.</p>"},{"location":"building-blocks/cloud-infrastructure/#elasticity","title":"Elasticity","text":"<p>One of the most significant advantages of this cloud infrastructure is its inherent elasticity, enabled by the virtual nature of its components. Services can be provisioned, scaled, or modified on demand through API integrations. This elasticity is not just a feature but a fundamental aspect, crucial for adapting to the fluctuating resource requirements inherent in the dynamic environment of the ALTERNATIVE platform. It allows for a responsive and agile infrastructure setup, capable of efficiently adapting to varying workloads and evolving project needs.</p> <p></p>"},{"location":"building-blocks/extensions/","title":"ALTERNATIVE platform extensions","text":"<p>The CKAN extension mechanism serves as a powerful framework for enhancing and customizing the functionality of the CKAN open-source data management system. CKAN's extensibility allows developers to augment the platform's core features, tailoring it to specific needs and requirements. Extensions can be created to introduce new functionalities, improve visualization capabilities, or customize branding elements. This extensibility is particularly valuable in the context of the ALTERNATIVE cloud data platform, where CKAN serves as the foundational technology.</p> <p>The following extensions were developed and implemented as part of the Task 6.6 \u201cImplementation of Cloud Data Platform\u201d.</p> name description ckanext-alternative_theme extension that changes the default theme of the platform ckanext-cloudstorage extension that implements support for S3 Cloud Storage. This is a modified version of the ckanext-cloudstorage extension, which uses libcloud ckanext-keycloak_auth extension that enables Keycloak authentication and user management ckanext-extrafields extension that adds additional fields to dataset metadata, such as size and experiment info ckanext-keycloak_access_token extension that enables management of AI/ML API tokens in Keycloak"},{"location":"building-blocks/features/","title":"Features","text":""},{"location":"building-blocks/features/#data-sharing-in-ckan","title":"Data sharing in CKAN","text":"<p>CKAN facilitates secure data sharing and exchange between permitted users and groups through its rich access control capabilities. All datasets and resources uploaded to CKAN can be assigned granular privileges dictating which users or organizations can view, access for analysis, edit metadata, or fully delete the assets.</p> <p>Flexible organization hierarchies allow reflective sharing permissions, such as a folder visible to members of a specific department, or an analytics project workspace shared between different users collaborating tightly. Access requests can also be configured for privileged approvals before enabling wider dataset visibility.</p> <p>Published open data is made publicly findable and accessible to all CKAN visitors. Private data restricted to members of the same group or organization can be easily shared for collaboration scenarios with audit logs tracking all access. Restricted datasets also benefit from CKAN features like ratings/comments, metadata enhancements, and discussions while remaining protected.</p>"},{"location":"building-blocks/features/#ckan-metadata","title":"CKAN Metadata","text":"<p>Metadata helps organize, find, and understand information better, acting as a helpful guide that tells you what you need to know about the stuff you're looking at or working with. CKAN provides a powerful metadata functionality, including default fields, custom fields and metadata extension for scientific experiments.</p> <ul> <li>Default Metadata Fields:</li> </ul> name description name unique across the platform, should be brief but specific URL automatically filled based on title, but can be edited Description longer description of the dataset, including information such as what people need to know when using the data Tags tags that help people find the data linked with other related data License it is important to include a license, so that people know how they can use the data Organization organization, owner of the dataset Visibility public or private visibility of the dataset Source where the data is from Version version of the data Author name of the person or organization responsible for producing the data Author email an email address of the author, to which queries about the data should be sent Maintainer name of the person or organization responsible for maintaining the data Maintainer email details for a second person responsible for the data <ul> <li>Advanced Metadata for Experiments: Because the project and datasets specifics related to experiment data, predefined metadata fields were implemented. When creating or editing a dataset a checkbox located above the custom metadata fields is available to show these extra fields.</li> </ul> name description Culture medium select culture medium between 2 options (EBM2 or Maintenance) Number of replicates set number of replicates Number of cells/well set number of cells/well Ratio iPSC-CMs/HCAECs set ratio iPSC-CMs/HCAECs Date of experiment the date of the experiment Toxin select toxin between 4 options (Ami, Blank, Dox, TBBPA) Age type select age type between 2 options (Old or Young) Dimension select dimension between 2 options (2D or 3D) Category select category between 6 options (Chip sensors, In vitro assays, Metabolomics, Proteomics, Toxin targeted metabolomics, Transcriptomics) Content select content between 3 options (Cells, Culture media, Not applicable) Model select model between 2 options (Dynamic or Static) <ul> <li>Custom Metadata Fields: Storing additional metadata for a dataset beyond the default metadata is a common use case. The ALTERNATIVE platform provides a simple way to do this by allowing you to store arbitrary key/value pairs against a dataset when creating or updating the dataset. These appear under the \u201cAdvanced metadata for experiments\u201d section on the web interface and in the \u2018extras\u2019 field of the JSON when accessed via the API.</li> </ul>"},{"location":"building-blocks/features/#development-environment-based-on-jupyterhub","title":"Development environment based on JUPYTERHUB","text":"<p>JupyterHub is leveraged to provide a multi-user development environment for data scientists and engineers collaborating on analytical workloads. It provides on-demand access to ephemeral notebook servers and IDEs, configured dynamically with language kernels, libraries, attached storage volumes, and access privileges depending on the user and context.</p> <p>Key advantages include scalability, as JupyterHub handles provisioning secure single-user workspaces on a Kubernetes cluster to serve users simultaneously. The workspace isolation also creates personalization allowing custom tool and dependency configuration per project needs. JupyterHub provides a rich interface over kernels like Python, R, and Scala for code, visualizations, documentation while integrating with version control and data platforms.</p> <p>Overall JupyterHub enables self-service access data science workbenches with security, governance, and devops service delivery advantages through Kubernetes backend automation. Notebooks can be easily shared between users and published to broader teams as standardized workflows or ML models are created. The workspace environment keeps evolving via community-sourced JupyterHub extensions too.</p>"},{"location":"building-blocks/features/#multi-user-collaboration-support","title":"Multi-user &amp; collaboration support","text":"<p>Supporting secure collaboration across diverse teams is a key requirement shaping the ALTERNATIVE platform software architecture and component integrations. The ability for project researchers, partner company scientists, and designated regulators to seamlessly access permitted datasets or analytical workflows facilitates improved transparency and oversight over model development stages.</p> <p>Additional collaboration-enhancing platform capabilities span multi-tenant Jupyter notebooks with selective content visibility and CKAN extension points to customize experiences for different user groups.</p>"},{"location":"building-blocks/features/#dataset-collaborators","title":"Dataset Collaborators:","text":"<p>In the Collaborators tab there is a list of all users that have been given special permission to the dataset (collaborators don't have to be members of the organization that owns the dataset). From the list the collaborators can be deleted or their role changed. Here are the collaborator\u2019s roles and rights:</p> <ul> <li>Member - can access the dataset if private, but not edit it</li> <li>Editor - can edit the dataset and its resources, as well as accessing the dataset if private</li> <li>Admin - in addition to managing the dataset, they can add and remove collaborators</li> </ul>"},{"location":"building-blocks/features/#single-sign-on-sso","title":"Single sign on - SSO","text":"<p>A key integration between the CKAN-based ALTERNATIVE web application and the JupyterHub development environment is single sign-on (SSO) based on Keycloak. From the web UI, users can launch Jupyter workspaces without reauthentication, providing a seamless user experience.</p>"},{"location":"building-blocks/kubernetes/","title":"Kubernetes","text":"<p>The ALTERNATIVE platform strategically employs a managed Kubernetes service, sourced from the selected cloud provider. This choice ensures a seamless integration between the cloud infrastructure and the Kubernetes environment, creating a cohesive and efficient operational framework.</p> <p>Kubernetes serves as the foundational target platform for all administrative and user-facing applications within the ALTERNATIVE ecosystem. It functions as a critical abstraction layer, effectively virtualizing and simplifying access to essential resources such as networking, storage, and compute capabilities. This abstraction not only streamlines resource management but also significantly enhances the user and administrator experience by abstracting the complexity of underlying infrastructure.</p> <p>One of the key strengths of using Kubernetes in this context is its inherent ability to provide seamless scalability. This feature is crucial for the platform, allowing it to dynamically adjust resource utilization in response to actual demand. Such scalability ensures that the platform can efficiently grow or shrink in resource usage, aligning closely with the varying requirements of the platform's workload.</p> <p>Furthermore, Kubernetes facilitates cost-effective resource utilization. By enabling the platform to scale resources in accordance with real-time demand, it ensures that the infrastructure is neither underutilized nor overburdened. This dynamic scalability not only optimizes performance but also contributes to a more economical use of resources, which is particularly beneficial in a cloud computing environment where resource allocation directly impacts operational costs. The needed auto scaling configuration has been implemented in the managed Kubernetes service.</p> <p>In summary, the integration of a managed Kubernetes service is a pivotal aspect of the ALTERNATIVE platform's architecture. It provides a robust, scalable, and cost-effective solution for managing the diverse array of applications and services, underpinning the platform's ability to meet current and future demands efficiently.</p>"},{"location":"building-blocks/storage/","title":"Storage","text":"<p> Figure 2: ALTERNATIVE cloud storage architecture</p> <p>The storage architecture within the ALTERNATIVE platform is especially designed to provide robust, scalable, and persistent data storage solutions. This is achieved by leveraging Kubernetes' powerful storage abstractions, namely <code>StorageClass</code>, <code>Persistent Volume Claim (PVC)</code>, and <code>Volume</code>. These abstractions play a crucial role in decoupling the applications from the underlying storage infrastructure, thereby enhancing flexibility and scalability.</p>"},{"location":"building-blocks/storage/#dynamic-allocation-of-data-volumes","title":"Dynamic Allocation of Data Volumes","text":"<p>The platform utilizes the dynamic provisioning capabilities of Kubernetes to allocate data volumes as and when needed. This approach ensures that storage resources are efficiently utilized, with volumes being created on-the-fly to meet the demands of the applications. It eliminates the need for pre-provisioning storage, thereby optimizing resource allocation and reducing overhead.</p>"},{"location":"building-blocks/storage/#persistence-and-reattachment","title":"Persistence and Reattachment","text":"<p>A key feature of the platform's storage strategy is the persistence of data, which is crucial for maintaining data integrity and continuity. Data volumes are designed to be persistent, meaning that the data remains intact even if the associated application is terminated or fails. These volumes can be seamlessly reattached to other instances of the application at a later point, ensuring data continuity and minimizing downtime.</p>"},{"location":"building-blocks/storage/#delegation-of-scalability-and-recovery","title":"Delegation of Scalability and Recovery","text":"<p>The platform delegates the responsibilities of scalability, recovery, and persistence to the cloud provider. By leveraging the cloud provider's robust infrastructure and services, the platform benefits from high scalability and efficient recovery mechanisms. This delegation allows the platform to focus on its core functionalities while relying on the cloud provider for maintaining the resilience and scalability of the storage infrastructure.</p>"},{"location":"building-blocks/storage/#types-of-storage-used","title":"Types of Storage Used","text":"<ul> <li>Block Storage: The platform employs block storage as the primary file system for applications. This type of storage is ideal for scenarios where performance and low-latency are critical. Block storage provides the applications with high-speed access to their data, making it suitable for a wide range of workloads.</li> <li>S3 Storage: For the CKAN component of the platform, S3 storage is utilized. S3, known for its scalability and durability, is an object storage service that offers a simple and cost-effective solution for storing and retrieving large amounts of data. This choice of storage aligns well with CKAN's design goal of handling large datasets.</li> </ul>"},{"location":"building-blocks/storage/#s3-storage","title":"S3 Storage","text":"<p>The ALTERNATIVE platform, particularly its CKAN component, has specific storage requirements due to the potentially large datasets containing omics data. To effectively manage these datasets, the platform employs S3 storage, a solution that is well-suited to handle large volumes of data.</p>"},{"location":"building-blocks/storage/#scalability-and-cost-effectiveness-of-s3","title":"Scalability and Cost-Effectiveness of S3","text":"<p>S3 storage is chosen for its exceptional scalability and cost-effectiveness. It is adept at handling vast amounts of data, scaling seamlessly in response to the platform's storage needs. This scalability ensures that as the volume of omics data grows, the storage infrastructure can grow correspondingly without any significant challenges. Additionally, S3's pricing model, which typically involves paying only for the storage used, makes it a cost-effective solution for managing large datasets.</p>"},{"location":"building-blocks/storage/#custom-ckan-integration-with-s3","title":"Custom CKAN Integration with S3","text":"<p>The integration of CKAN with S3 storage has been achieved through a custom CKAN extension. This tailored extension enables CKAN to efficiently interface with the S3 storage, ensuring that the data storage and retrieval processes are optimized for performance and reliability.</p>"},{"location":"building-blocks/storage/#direct-interface-with-s3","title":"Direct Interface with S3","text":"<p>In the ALTERNATIVE platform, CKAN interfaces directly with the S3 storage, bypassing integration via the Kubernetes plane. This direct interaction streamlines the data flow between CKAN and S3, reducing complexity and potential bottlenecks that might arise from additional layers of integration.</p>"},{"location":"building-blocks/storage/#s3-storage-service-and-minio-library","title":"S3 Storage Service and Minio Library","text":"<p>The S3 storage service, provided by the cloud provider, utilizes the Minio library. Minio is an open-source object storage server that provides a high-performance, scalable solution and is fully compatible with Amazon S3 APIs. The use of Minio enhances the platform's ability to interact with S3 storage efficiently and securely.</p>"},{"location":"building-blocks/storage/#ckans-api-based-data-exposure","title":"CKAN's API-based Data Exposure","text":"<p>It is important to note that CKAN does not directly expose the underlying S3 storage to end users. Instead, CKAN provides its own set of APIs for data access. This approach ensures that data access is controlled and secure, with CKAN serving as the intermediary layer managing user interactions with the stored data. This setup not only enhances security but also allows for the implementation of additional features and controls over how the data is accessed and used.</p> <p>In summary, the use of S3 storage in the ALTERNATIVE platform, particularly for CKAN, is a strategic decision that addresses the specific needs of large-scale omics data management. The custom integration of CKAN with S3, the direct interface approach, and the utilization of the Minio library collectively ensure that the platform can handle large datasets efficiently while maintaining cost-effectiveness and security.</p>"},{"location":"building-blocks/storage/#block-storage","title":"Block Storage","text":"<p>In the ALTERNATIVE platform, block storage is the main type of data storage utilized by applications. This section delves into the specifics of how block storage is implemented and managed within the platform's architecture.</p>"},{"location":"building-blocks/storage/#exposure-as-file-system","title":"Exposure as File System","text":"<p>Block storage is exposed to applications and services as a file system. This approach is familiar and intuitive for application developers, allowing them to interact with the storage using standard file operations. This method of exposure simplifies the process of reading from and writing to the storage medium, making it highly accessible for a variety of applications.</p>"},{"location":"building-blocks/storage/#underlying-storage-system","title":"Underlying Storage System","text":"<p>Block storage systems are most commonly disk-based, utilizing either Hard Disk Drives (HDD) or Solid State Drives (SSD). The choice between HDD and SSD is typically based on a trade-off between cost and performance. SSDs, with their faster data access speeds, are well-suited for high-performance requirements, whereas HDDs are often used for cost-effective storage solutions where speed is less critical.</p>"},{"location":"building-blocks/storage/#consumption-via-kubernetes-apis","title":"Consumption via Kubernetes APIs","text":"<p>Block storage within the platform is consumed through Kubernetes APIs, specifically through the use of volumes, Persistent Volume Claims (PVCs), and Storage Classes. This integration with Kubernetes APIs ensures a seamless and efficient management of storage resources within the platform's ecosystem. The platform offers the flexibility to define different storage classes, each tailored to meet varying performance requirements. This feature allows for the customization of storage resources based on specific needs of different applications or services. For instance, a storage class optimized for high I/O throughput can be used for data-intensive applications, while another class might be optimized for cost-effectiveness.</p>"},{"location":"building-blocks/storage/#decoupling-of-applications-from-storage","title":"Decoupling of Applications from Storage","text":"<p>A significant advantage of using block storage in this manner is the decoupling of applications from the physical storage. This abstraction is particularly beneficial in scenarios such as backup, disaster recovery, and migration. By abstracting the storage layer, applications can be easily moved, backed up, or restored without the need to manage the complexities of the underlying storage infrastructure. This decoupling not only enhances the flexibility of the storage system but also significantly improves the platform's resilience and agility in handling data.</p>"},{"location":"building-blocks/storage/#nfs","title":"NFS","text":"<p>The platform deploys an NFS (Network File System) server additionally, to provide extra storage class for use by some applications. For instance, JupyterHub users require a shared data folder, which is used to collaborate on data available for multiple users at a time. This means that Jupyter user PODs need to attach to the same volume, which requires a storage class with mode <code>ReadWriteMany</code>. Since the cloud provider only offers block storage classes of type <code>ReadWriteOnce</code>, the NFS server is used to expose such volume.</p>"},{"location":"building-blocks/storage/#ckan-storage","title":"CKAN Storage","text":"<p>The CKAN application uses several types of storage: - Database: Relational database is used to store the application state, including metadata, user data. Postgres is used in this case. - S3: User defined data sets are mapped to S3 buckets. - Block storage: For regular files like configuration needed by CKAN.</p>"},{"location":"building-blocks/storage/#jupyterhub-storage","title":"JupyterHub Storage","text":"<p> Figure 3: Diagram of Jupyter Storage</p> <p>JupyterHub has more complex storage requirements compared to the other ALTERNATIVE services.</p>"},{"location":"building-blocks/storage/#multi-user-capability","title":"Multi-user capability","text":"<p>Each user is dynamically allocated a Kubernetes POD which contains the Jupyter kernel and runtime for this user. The POD is ephemeral, but its data associated with the user must be persisted to a user volume, which is 10 GB. Each user is allocated such volume the first time after login and when the POD disappears due to session timeout or logout of the user, the volume remains. When the POD is recreated it attaches to the same volume and replays the state from the last session.</p>"},{"location":"building-blocks/storage/#shared-folder","title":"Shared folder","text":"<p>Additionally, users request the feature to be able to share data directly in Jupyter via a shared folder. This folder is mapped to a static volume with size 20 GB and mode <code>ReadWriteMany</code>, because multiple PODs must be able to attach and write to it simultaneously. The <code>ReadWriteMany</code> volume is provided by the NFS server.</p>"},{"location":"building-blocks/storage/#accessing-ckan-data-from-jupyter","title":"Accessing CKAN data from Jupyter","text":"<p>A custom python library was developed for ALTERNATIVE users called <code>alternative-lib</code>. It simplifies the access to CKAN/S3 data from Jupyter and is hosted on GitHub as an open-source project.</p>"},{"location":"building-blocks/storage/#databases","title":"Databases","text":"<p>Postgres database is deployed within the Kubernetes plane. It is used by CKAN and Keycloak services. The Postgres leverages block storage underneath.</p>"},{"location":"building-blocks/storage/#code-repository","title":"Code repository","text":"<p>ALTERNATIVE uses GitHub as a code repository. A dedicated GitHub organization has been created with the following repositories.</p> <p>Table 1: List with ALTERNATIVE project code repositories in GitHub</p> Repository Name Description platform-deployment Deployment files and instructions for the platform ckanext-cloudstorage CKAN extension that implements support for S3 Cloud Storage ckanext-keycloak_auth CKAN extension that enables Keycloak authentication and user management ckanext-alternative_theme CKAN extension that changes the default theme of the platform to match the look of ALTERNATIVE alternative.github.io Documentation about the ALTERNATIVE platform ckanext-extrafields CKAN extension that adds additional fields to dataset metadata, such as size and experiment info alternative-lib Python library to work with the Alternative platform"},{"location":"building-blocks/user-services/","title":"User Services","text":""},{"location":"building-blocks/user-services/#user-services","title":"User services","text":"<p>The user services are those used by users to interact directly with. Those include CKAN and JupyterHub.</p>"},{"location":"building-blocks/user-services/#ckan","title":"CKAN","text":"<p>The ALTERNATIVE platform\u2019s data sharing part is based on CKAN, a web-based catalog platform for working with large datasets in a collaborative style. CKAN provides an easy to use web interface, which can be used to create, share and manage various types of data either with internal teams or external audiences. </p> <p>Users, Organizations, and Authorization The CKAN platform, integral to the ALTERNATIVE project, categorizes users into two distinct types: regular users and sysadmin users. The sysadmin users are vested with the authority to create organizations within the platform. Upon creation, an organization initially contains no datasets and has a single member, typically the user who established it. While the platform allows unregistered users to search for and access public data, registration is mandatory for engaging in publishing activities and accessing personalization features. The management of user identity and access within the CKAN platform is efficiently handled by Keycloak.</p> <p>In the context of the ALTERNATIVE platform, each consortium partner is represented as an organization. These organizations possess unique workflows and authorization mechanisms, enabling them to autonomously manage their respective publishing processes. Organizations serve as the fundamental unit for controlling access to datasets. They determine who can view, create, and modify datasets within the platform. The administrative personnel within an organization have the capability to add users and assign them specific roles, each with varying levels of access and control:</p> <ul> <li>Member Role: Users with this role have the privilege to view private datasets owned by the organization.</li> <li>Editor Role: In addition to all privileges available to a Member, Editors can edit and publish datasets.</li> <li>Admin Role: Users in this role encompass all capabilities of an Editor, with the added responsibilities of adding, removing, and altering the roles of organization members.</li> </ul> <p>This structured approach to user roles and organization management within the ALTERNATIVE platform ensures a streamlined and secure process for dataset management and publication. It allows for a tailored and controlled environment, where each organization can maintain the integrity and confidentiality of its data while facilitating collaboration and data sharing as per the project's objectives.</p> <p></p> <p>Datasets, Resources and Groups The ALTERNATIVE platform handles data publication through \"datasets.\" Each dataset is a compilation of metadata describing the data, accompanied by various resources that contain the actual data. The platform is designed to be agnostic to the data format, accommodating a diverse range of resource types including, but not limited to, CSV or Excel files, XML files, PDF documents, image files, and linked data in RDF format. Resources are primarily stored in an S3 bucket within Cloud Storage, although they can also exist as external web links.</p> <ul> <li>Dataset Exploration: The platform provides a comprehensive interface for exploring datasets. Users can view a complete list of available datasets through the datasets menu. Access to a specific dataset is facilitated either from this list or via an organization's page. Upon selection, the dataset page is displayed, which is organized into three main tabs:<ul> <li>The 'Dataset' tab presents detailed information about the dataset, including a list of its resources. Selecting a resource redirects the user to a dedicated page where details about the resource are available for review, management, and download.</li> <li></li> <li>The \u2018Groups\u2019 tab for managing which groups of users can access the dataset.</li> <li>The 'Activity Stream' tab chronicles the historical changes made to the dataset, providing a transparent audit trail.</li> </ul> </li> <li>Dataset Creation: The process of creating a new dataset on the ALTERNATIVE platform is designed to be intuitive. Users can initiate dataset creation either from the 'Datasets' page by clicking the 'Add Dataset' button or through the 'Organisations' page by selecting the appropriate owning organization.</li> <li></li> <li>Dataset Management: Dataset management privileges are granted based on user roles and associations. A user can manage any dataset they have created, any dataset owned by an organization they are a member of, or any dataset where they are designated as a collaborator with at least an Editor role.</li> <li>Resource Management within a Dataset: Managing a dataset's resources is facilitated through the 'Resources' tab. Users can add new resources to a dataset using the 'Add new resource' button. Existing resources can be edited by selecting the resource and modifying its information. All changes are finalized and saved using the 'Update Resources' button.</li> <li>Metadata: Each dataset contains metadata, which can be defined during creation of at a later point. This metadata contains descriptive information like author, organization, license and others, which can be used when querying the platform\u2019s datasets. For the purposes of ALTERNATIVE, CKAN has been extended via custom metadata fields grouped under \u201cAdvanced metadata for experiments\u201d, which can be additionally defined for datasets which are part of ALTERNATIVE\u2019s scientific base.</li> </ul> <p></p>"},{"location":"building-blocks/user-services/#jupyterhub","title":"JupyterHub","text":"<p>JupyterHub is seamlessly integrated into the ALTERNATIVE platform, providing researchers and scientists an integrated development environment for working with datasets and analytical workflows.</p> <p>Upon accessing the JupyterHub single sign-on URL from the web portal, authenticated users have a dedicated JupyterHub server spawned dynamically for them. This creates a personalized workspace with a rich set of tools encompassing Jupyter notebooks, text editors, terminals, code consoles, file browsers, data visualization, version control integration and more.</p> <p>Jupyter Notebooks in particular are versatile documents integrating executable code, mathematical equations, graphics and interactive visualizations into a computational scratchpad. This environment facilitates both exploratory analysis over datasets available to the user as well as rapid development of reusable scripts, machine learning pipelines, models and applications by coding in languages like Python and R.</p> <p>To optimize resource usage, any ephemeral notebooks servers are automatically shutdown after periods of user inactivity, while keeping any workflow outputs and snapshots preserved for future access. This balances providing responsive on-demand environments with managing platform costs efficiently at scale.</p> <p>JupyterHub itself provides extensive customization options accessible after the server is provisioned for a user. Common configurations include: - File browser pane for data and workspace asset access control integrations - Tabbed based editor for concurrently working across documents like notebooks, code files, markdown reports etc - Menu of terminals to manage dependencies or launch Docker environments as needed - Sidebar of tools ranging from version control plugins, debugger, data visualization toolkit and extensions catalog - Global user preferences from UI behavior to notebooks runtimes to visualize resource utilization - Keyboard shortcuts for expedited navigation and frequent tasks - Hub control panel reconnect back to central management consoles - Github integration plugin in the sidebar</p> <p>For illustrative purposes, examined below is a standard data science focused Jupyter notebook making use of many integrated capabilities:</p> <p></p> <p>Every JupyterHub server is being spawned with two environments from the start - default python environment and an additional conda environment. On the Launcher tab under Notebook or Console there are options to choose which environment to use. The Terminal also can be used:</p> <ul> <li>Python Virtual Environments - Each environment has their own independent set of Python packages installed in their site directories. A virtual environment is created on top of an existing Python installation, known as the virtual environment\u2019s base Python, and may be isolated from the packages in the base environment, so only those explicitly installed in the virtual environment are available.</li> <li>Conda Environments - Each environment has their own independent set of Python or Conda packages installed.</li> <li>Python Library - The python library alternative-lib is designed to help with finding datasets and downloading resources from them, by using ckanapi.</li> <li>R - is a free software environment for statistical computing and graphics. In the ALTERNATIVE platform the R environment can be used from the Terminal.</li> <li>Bioconductor Integration - Bioconductor is a free toolkit that helps scientists study genes and DNA data. It offers lots of tools for analysing and visualising this information, making it easier for researchers to understand complex genetic data. People from all over the world work together to make Bioconductor better, so scientists can keep making new discoveries in genetics. Bioconductor relies on the R programming language and follows open-source principles, welcoming contributions from anyone. It's updated twice a year and has a vibrant user community. Additionally, Bioconductor can be accessed through Docker images for convenient use.</li> </ul>"},{"location":"building-blocks/user-services/#aiml-api","title":"AI/ML API","text":"<p>The AI/ML API is a key component of the ALTERNATIVE platform, providing users with access to a range of machine learning models and algorithms. These models are designed to support various use cases, including data analysis, predictive modeling, and decision-making. The API is built on top of a scalable and efficient infrastructure, allowing users to run complex machine learning tasks with ease. The AI/ML API is accessible through a set of RESTful endpoints, which can be integrated into existing applications or used directly by users.</p>"}]}